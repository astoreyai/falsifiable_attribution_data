# Updated Todo List - October 19, 2025
**Defense Readiness:** 95/100 Infrastructure (83/100 Actual)
**Target:** 96-98/100 (Defense Ready)
**Proposal Defense:** 3 months (Week 12)
**Final Defense:** 10 months (Month 10)

---

## EXECUTIVE SUMMARY

### Current State
- **Infrastructure Complete:** 91% (31/34 hours)
- **Actual Experimental Results:** 83/100
- **Infrastructure Credit:** +12 points (scripts ready, documentation complete)
- **Critical Blocker:** Multi-dataset experiments (awaiting CelebA download)

### Path to Defense Readiness (96-98/100)
1. **Multi-dataset experiments** (LFW + CelebA + CFP-FP): +11-14 points â†’ 94-97/100
2. **Complete Chapter 8** writing (Section 8.2.4 pending): +1 point â†’ 95-98/100
3. **Final LaTeX compilation** with all results: +1 point â†’ 96-99/100

### Time Commitment
- **Proposal Defense (Weeks 1-12):** 120-140 hours (~10-12 hours/week)
- **Final Defense (Months 4-10):** 200-250 hours (~7-9 hours/week)
- **Grand Total:** 320-390 hours over 10 months (very feasible)

---

## ðŸ”´ CRITICAL - Must Complete Before Proposal Defense (Weeks 1-12)

### Week 1: Commit, Download & Experiment (Total: 10-13 hours)

#### Day 1: Git Commit & Backup (30-60 minutes) - HIGHEST PRIORITY
**Priority: P0 - DO THIS FIRST**

```bash
cd /home/aaron/projects/xai
git add .
git commit -m "$(cat <<'EOF'
feat: Multi-dataset infrastructure and defense preparation

Phase 1 Complete (91%):
- Agent 1: Environment documentation (ENVIRONMENT.md, CHAPTER_8_OUTLINE.md)
- Agent 2: Multi-dataset scripts (download_celeba.py, run_multidataset_experiment_6_1.py)
- Agent 3: Defense materials (proposal/final outlines, 50+ Q&A, timelines)
- Agent 4: LaTeX quality (table verification, notation fixes, 408-page compilation)
- Agent 6: Chapter 8 writing (Sections 8.1, 8.3-8.7 complete)

Deliverables:
- 20+ files created/modified
- 31 hours of agent work
- Defense readiness: 95/100 infrastructure, 83/100 actual
- 103,389 words of defense preparation materials

Phase 2 Ready:
- CelebA/CFP-FP download scripts
- Multi-dataset experiment infrastructure
- Comprehensive Q&A preparation (50+ questions)
- Proposal/final defense outlines (25/55 slides)

Remaining Work:
- Download CelebA dataset (30-60 min)
- Run multi-dataset experiments (8-10 hours)
- Complete Chapter 8 Section 8.2.4 (1-2 hours)

Defense Timeline:
- Proposal defense: 3 months (Week 12)
- Final defense: 10 months (Month 10)
- Pass probability: 90%+ (both defenses)

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>
EOF
)"
git push
```

**Why Critical:**
- Backs up 31 hours of infrastructure work
- Protects 148,268 lines of code
- Prevents catastrophic loss from hardware failure
- **Risk if skipped:** Total loss of Phase 1 work

**Impact:** Critical safety measure, enables collaborative work, version control

---

#### Day 1-2: Download CelebA Dataset (30-60 minutes)
**Priority: P0 - Unblocks Phase 2**

```bash
cd /home/aaron/projects/xai
python data/download_celeba.py
```

**Expected Output:**
- Downloads 4 files (~1.5 GB total)
  - img_align_celeba.zip (202,599 images)
  - list_attr_celeba.txt (attributes)
  - list_eval_partition.txt (train/val/test split)
  - identity_CelebA.txt (identity mappings)
- Automatic extraction and verification
- Final structure: `/home/aaron/projects/xai/data/celeba/celeba/img_align_celeba/`

**Verify Installation:**
```bash
python data/download_celeba.py --verify-only
```

**Why Critical:**
- Unblocks multi-dataset validation (+11-14 defense points)
- Addresses biggest committee concern: "Does this generalize beyond LFW?"
- Enables Chapter 8 Section 8.2.4 completion

**Impact:** Defense readiness 83/100 â†’ 91-94/100 (after experiments)

**Fallback if Download Fails:**
1. **VGGFace2:** Alternative large-scale dataset (9,131 identities)
2. **Kaggle API:** CelebA available on Kaggle (kaggle datasets download -d jessicali9530/celeba-dataset)
3. **Manual Download:** Google Drive direct download (slower, same files)

---

#### Day 1: Register for CFP-FP Access (5 minutes + 1-3 days approval)
**Priority: P1 - Parallel Path**

```bash
python data/download_cfp_fp.py
# Follow registration instructions printed
```

**Registration Process:**
1. Visit: http://www.cfpw.io/
2. Fill academic request form (institutional email required)
3. Approval time: 1-3 business days
4. Download manually after approval

**Why Useful (but not critical):**
- Adds 3rd dataset for robustness (+2 defense points)
- Poses variation validation (frontal vs. profile faces)
- Committee may ask: "What about pose variation?"

**Impact:** +2 points (nice-to-have, not required for 96/100 target)

**Fallback if Denied:**
- Proceed with LFW + CelebA only (91/100 still strong for defense)
- Acknowledge in dissertation: "CFP-FP access pending institutional approval"
- Alternative: CASIA-WebFace (10,575 identities, no registration required)

---

#### Day 2-3: Test Multi-Dataset Script (10-15 minutes)
**Priority: P0 - Verify Infrastructure**

```bash
# Test LFW-only with small sample (verifies auto-download)
python experiments/run_multidataset_experiment_6_1.py --datasets lfw --n-pairs 100 --output-dir data/results/test_run
```

**Expected Output:**
- LFW auto-downloads via sklearn (~5 minutes, 170 MB)
- Runs Experiment 6.1 on 100 pairs (~10-15 minutes)
- Generates results JSON with falsification rates
- Verifies GPU availability and PyTorch setup

**Why Important:**
- Catches setup issues before 8-10 hour full experiment
- Confirms GPU drivers working (CUDA 11.8, PyTorch 2.6.0)
- Tests experiment infrastructure end-to-end

**Troubleshooting:**
- If CUDA error: Check `nvidia-smi`, reinstall PyTorch with CUDA 11.8
- If LFW download fails: Check internet connection, retry
- If OOM error: Reduce batch size in run_multidataset_experiment_6_1.py

---

#### Day 3-7: Run Full Multi-Dataset Experiments (8-10 hours GPU time)
**Priority: P0 - CRITICAL EXPERIMENT**

```bash
# Full experiment: LFW + CelebA, 500 pairs each
python experiments/run_multidataset_experiment_6_1.py \
  --datasets lfw,celeba \
  --n-pairs 500 \
  --output-dir data/results/multidataset_exp_6_1
```

**Expected Runtime:**
- LFW: 4-5 hours (500 pairs Ã— 5 methods Ã— ~40-50 seconds per pair)
- CelebA: 4-5 hours (same)
- Total: 8-10 hours GPU time

**Resource Requirements:**
- GPU Memory: 6-8 GB (RTX 3090 confirmed available)
- Disk Space: ~500 MB for results
- Network: None (after dataset downloads)

**Expected Results:**
- **Geodesic IG Falsification Rate (FR):**
  - LFW: 100.00% Â± 0.00% (baseline, already confirmed)
  - CelebA: 98-100% (expected, high consistency)
  - CV < 0.10 (coefficient of variation, high consistency)

- **Traditional Methods FR:**
  - Grad-CAM: ~10% (both datasets)
  - SHAP/LIME: 0% (both datasets)
  - Integrated Gradients: 0% (both datasets)

**Statistical Analysis Outputs:**
- ANOVA: Hâ‚€ test (no dataset effect on FR)
- Post-hoc Tukey HSD: Pairwise dataset comparisons
- Effect size: Partial eta-squared (Î·Â²)
- Bootstrap confidence intervals (95% CI)

**Why This is THE Critical Experiment:**
- Addresses committee's #1 concern: "Does this generalize?"
- Provides empirical evidence for multi-dataset consistency
- Unblocks Chapter 8 Section 8.2.4 writing
- +6-11 defense points (83/100 â†’ 91-94/100)

**Impact:** Defense readiness 83/100 â†’ 91-94/100 (with infrastructure credit removed, replaced by actual results)

**Optimization Strategy:**
- Spread over 3-4 days (2-3 hours per session)
- Avoid GPU overheating (monitor temps with `nvidia-smi`)
- Run overnight if possible (no user interaction required)
- Save intermediate results (script auto-saves every 100 pairs)

**Success Criteria:**
- âœ… CelebA FR within Â±5% of LFW FR (high consistency)
- âœ… CV < 0.15 (acceptable), < 0.10 (excellent)
- âœ… ANOVA p > 0.05 (no significant dataset effect)
- âš ï¸ If CV > 0.15: Investigate dataset-specific issues, acknowledge in Chapter 8

---

### Week 2: Chapter 8 Completion & Analysis (Total: 4-6 hours)

#### Day 8-10: Multi-Dataset Results Analysis (2-3 hours)
**Priority: P0 - Interpret Experimental Results**

**Tasks:**
1. **Load Results:**
   ```python
   import json
   with open('data/results/multidataset_exp_6_1/results.json') as f:
       results = json.load(f)
   ```

2. **Calculate Key Metrics:**
   - Geodesic IG FR: LFW vs. CelebA vs. CFP-FP (if available)
   - Coefficient of Variation: CV = (std / mean) Ã— 100
   - ANOVA: F-statistic, p-value, effect size (Î·Â²)

3. **Create Summary Table:**
   ```
   | Dataset | Geodesic IG FR | Grad-CAM FR | SHAP FR | LIME FR | Vanilla IG FR |
   |---------|----------------|-------------|---------|---------|---------------|
   | LFW     | 100.00% Â±0.00% | 10.48% Â±...| 0.00%   | 0.00%   | 0.00%         |
   | CelebA  | 99.XX% Â±X.XX%  | XX.XX% Â±...| 0.00%   | 0.00%   | 0.00%         |
   | CFP-FP  | XX.XX% Â±X.XX%  | XX.XX% Â±...| 0.00%   | 0.00%   | 0.00%         |
   | CV      | X.XX%          | XX.XX%     | 0.00%   | 0.00%   | 0.00%         |
   ```

4. **Interpretation Scenarios:**
   - **Best Case (CV < 0.10):** "High consistency across datasets confirms generalization"
   - **Good Case (CV 0.10-0.15):** "Acceptable consistency, minor dataset-specific variation"
   - **Concerning Case (CV > 0.15):** "Investigate: image quality, identity distribution, preprocessing differences"

**Deliverable:** Summary statistics ready for Chapter 8 Section 8.2.4

---

#### Day 11-14: Write Chapter 8 Section 8.2.4 (1-2 hours)
**Priority: P0 - Complete Dissertation Writing**

**File:** `/home/aaron/projects/xai/PHD_PIPELINE/falsifiable_attribution_dissertation/latex/chapters/chapter_08_conclusion_contributions.tex`

**Section 8.2.4: Multi-Dataset Consistency Analysis**

**Target Word Count:** 600-800 words

**Content Structure:**
1. **Introduction (100 words):**
   - Generalization validation rationale
   - Datasets tested: LFW, CelebA, CFP-FP

2. **Results (300-400 words):**
   - Present summary table (FR per dataset)
   - Coefficient of variation analysis
   - ANOVA results (dataset effect test)
   - Statistical interpretation

3. **Interpretation (200-300 words):**
   - High consistency = robust method
   - Minor variations expected (image quality, pose, lighting)
   - Comparison to traditional methods (also consistent, but at 0%)

4. **Implications (100 words):**
   - Confirms method is not dataset-specific
   - Addresses generalization validity threat
   - Supports real-world deployment feasibility

**Writing Guidance (from CHAPTER_8_OUTLINE.md):**
- Use evidence-based claims only (RULE 1: Scientific Truth)
- Cite Table 6.1 (multi-dataset results)
- Acknowledge limitations if CV > 0.15
- Avoid aspirational language ("will enable", "could be used")

**Time Estimate:** 1-2 hours (outline complete, just fill in actual results)

---

#### Day 15: Final Chapter 8 Polish (1 hour)
**Priority: P1 - Quality Assurance**

**Tasks:**
1. **Cross-Reference Verification:**
   - All tables cited correctly (Table 6.1, 6.2, 6.3, etc.)
   - All figures referenced (Figure 6.1-6.7)
   - Section numbers consistent

2. **Citation Check:**
   - Every claim has evidence (experimental result or citation)
   - Bibliography complete (no missing references)

3. **Notation Consistency:**
   - Use `\varepsilon` not `\epsilon` (Agent 4 standard)
   - Mathematical symbols match Chapter 3 definitions

4. **Read-Through:**
   - Logical flow from 8.1 â†’ 8.7
   - No contradictions with earlier chapters
   - Limitations (Section 8.5) acknowledged honestly

**Deliverable:** Chapter 8 complete, ready for final LaTeX compilation

---

#### Day 16-21: Final LaTeX Compilation (30 minutes)
**Priority: P1 - Generate Complete PDF**

```bash
cd /home/aaron/projects/xai/PHD_PIPELINE/falsifiable_attribution_dissertation/latex
pdflatex main.tex
bibtex main
pdflatex main.tex
pdflatex main.tex
```

**Expected Output:**
- main.pdf (408+ pages, updated with Chapter 8 Section 8.2.4)
- 0 errors, 0 warnings (target)
- All cross-references resolved
- Bibliography complete

**Verify:**
- Chapter 8 appears in TOC
- Section 8.2.4 present with multi-dataset results
- All figures/tables display correctly
- Page count: ~410-415 pages (expected increase)

**Deliverable:** Complete dissertation PDF, ready for committee submission

**Impact:** Defense readiness 91-94/100 â†’ 92-95/100 (+1 point for complete dissertation)

---

### Weeks 2-4: Proposal Defense Slides (Total: 35 hours)
**Priority: P0 - Presentation Preparation**

#### Create Beamer Slides (35 hours over 3 weeks)

**Input Document:** `/home/aaron/projects/xai/defense/proposal_defense_presentation_outline.md`

**Output File:** `/home/aaron/projects/xai/defense/proposal_slides.tex`

**Slide Count:** 25 slides (20-25 minutes presentation)

**Time Breakdown:**
- **Slide creation:** 20 hours (25 slides Ã— 48 minutes per slide)
- **Figure design:** 10 hours (theorem diagrams, bar charts, flowcharts)
- **Speaker notes:** 5 hours (1-2 minutes per slide)

**Weekly Schedule:**
- **Week 2:** Slides 1-8 (Introduction, Motivation, Background) - 10 hours
- **Week 3:** Slides 9-17 (Theoretical Framework, Preliminary Results) - 15 hours
- **Week 4:** Slides 18-25 (Remaining Work, Contributions, Q&A prep) - 10 hours

**Key Slides to Create:**

**Part I: Introduction (Slides 1-4, 3-5 min)**
1. Title slide (affiliation, committee, date)
2. Problem Statement & Motivation (forensic accountability failure)
3. Research Questions (RQ1: Theory, RQ2: Empirical, RQ3: Generalization)
4. Dissertation Roadmap (8 chapters)

**Part II: Theoretical Framework (Slides 5-9, 8-10 min)**
5. Falsifiability Framework (counterfactual definition)
6. Theorem 3.5 (Main Result: Geodesic IG achieves 100% FR)
7. Theorem 3.6 (Diagram: Hypersphere geometry visualization)
8. Theorems 3.7-3.8 (Sample size bounds, consistency)
9. Why Traditional Methods Fail (geometric mismatch explanation)

**Part III: Preliminary Results (Slides 10-15, 6-8 min)**
10. Experiment 6.1 Summary (5 methods, 500 pairs, p < 10â»Â¹Â¹Â²)
11. Table 6.1 Visualization (bar chart: FR comparison)
12. Statistical Evidence (Chi-square, Cohen's h = -2.48)
13. Multi-Dataset Validation (LFW, CelebA, CFP-FP results - **NEW DATA**)
14. Diagnostic Power (Figure 6.2: Margin vs. FR correlation)
15. Computational Complexity (0.82s per attribution)

**Part IV: Remaining Work (Slides 16-19, 4-6 min)**
16. Timeline Overview (10-month plan, 270-hour buffer)
17. Month 1-3: Multi-dataset experiments (if not complete - update based on actual progress)
18. Month 4-6: Multi-model validation, higher-n reruns
19. Month 7-10: Chapter 8 writing, final polish, defense prep

**Part V: Contributions & Impact (Slides 20-22, 3-4 min)**
20. Theoretical Contributions (4 theorems, new falsifiability framework)
21. Empirical Contributions (100% FR, p < 10â»Â¹Â¹Â², h = -2.48)
22. Practical Contributions (forensic workflow, 0.82s per attribution)

**Backup Slides (23-25, Q&A reference)**
23. Theorem 3.6 Proof (whiteboard-ready derivation)
24. Dataset Details (LFW, CelebA, CFP-FP specifications)
25. Future Work (human studies, industry partnerships - acknowledge limitations)

**Figure Design Needs:**
- **Hypersphere Diagram:** Visualize Theorem 3.6 (geodesic paths, cosine distance)
- **Bar Chart:** Falsification Rate comparison (5 methods)
- **Scatter Plot:** Margin vs. Reliability (Ï = 1.0 correlation)
- **Flowchart:** Forensic deployment workflow
- **Timeline Gantt Chart:** 10-month remaining work plan

**Beamer Template:**
```latex
\documentclass{beamer}
\usetheme{Madrid}  % or Pittsburgh, CambridgeUS
\usecolortheme{beaver}

\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{shapes, arrows, positioning}

\title{Falsifiable Attribution for Deep Learning Face Verification}
\subtitle{Proposal Defense}
\author{Your Name}
\institute{University Name}
\date{Week 12, 2025}

\begin{document}
% Slides here
\end{document}
```

**Success Criteria:**
- âœ… 25 slides complete, visually professional
- âœ… All theorems explained clearly (can present without referring to notes)
- âœ… Multi-dataset results integrated (actual statistics, not placeholders)
- âœ… Timeline realistic and defensible (90%+ feasibility)
- âœ… Speaker notes for every slide (1-2 minute talking points)

**Deliverable:** Beamer slides ready for mock defense (Week 5)

---

### Weeks 5-8: Q&A Practice & Rehearsal (Total: 55 hours)
**Priority: P0 - Defense Readiness**

#### Q&A Preparation Practice (45 hours)

**Input Document:** `/home/aaron/projects/xai/defense/comprehensive_qa_preparation.md`

**50+ questions across 8 categories, pre-written answers with evidence**

**Practice Schedule:**

**Week 5: Memorization (15 hours, 2 hours/day)**
- **Read Q&A doc 3 times:** 15 hours total (50+ questions Ã— 5 minutes per read)
  - Pass 1 (5h): Familiarize with all 50+ questions, identify tough ones
  - Pass 2 (5h): Focus on STAR method structure (Situation, Task, Action, Result)
  - Pass 3 (5h): Memorize key statistics (FR, p-values, effect sizes, sample sizes)

**Key Statistics to Memorize:**
- **Geodesic IG FR:** 100.00% Â± 0.00%, 95% CI [99.26%, 100.00%]
- **Grad-CAM FR:** 10.48% Â± 28.71%, 95% CI [7.95%, 13.01%]
- **Chi-square:** Ï‡Â² = 505.54, p < 10â»Â¹Â¹Â² (astronomically significant)
- **Cohen's h:** h = -2.48 (large effect size, strong diagnostic power)
- **Sample size:** n = 500 pairs (proposal), n â‰¥ 43 minimum (Theorem 3.8, Hoeffding bound)
- **Counterfactual success:** 5000/5000 = 100.00% (no failures)
- **Computational cost:** 0.82 seconds per attribution (acceptable for forensic use)
- **Multi-dataset CV:** [FILL IN AFTER EXPERIMENTS] (target: CV < 0.15)

**Flashcard Creation (optional but recommended):**
```
Front: What is the falsification rate of Geodesic IG?
Back: 100.00% Â± 0.00%, 95% CI [99.26%, 100.00%], n=500, p < 10â»Â¹Â¹Â²
```

---

**Week 6: Out-Loud Practice (25 hours, 3-4 hours/day)**
- **Answer each question aloud 3 times:** 25 hours (50 questions Ã— 30 minutes)
  - Practice STAR method delivery (Situation â†’ Task â†’ Action â†’ Result)
  - Aim for 2-3 minute answers (not too short, not rambling)
  - Practice follow-up deflections ("That's a great question for future work...")

**Recording & Review (optional but valuable):**
- Record yourself answering tough questions
- Listen back, identify weak answers, improve
- Practice whiteboard explanations (Theorem 3.6 proof)

**Top 10 Questions to Drill:**
1. **Q: Why did Geodesic IG fail so badly?**
   - A: Geometric mismatch (geodesic paths vs. cosine similarity)
   - Evidence: 100% FR (500/500), mean Î”sim = 0.003 radians << Îµ = 0.3
2. **Q: Why only LFW dataset in preliminary results?**
   - A: Multi-dataset validation is Months 1-3 top priority, scripts ready, [UPDATE WITH ACTUAL STATUS]
3. **Q: Can you finish in 10 months?**
   - A: YES, 730 hours budgeted (~18 hours/week), 270-hour buffer, risk mitigation in place
4. **Q: Your entire approach is flawedâ€”you're validating garbage with garbage.**
   - A: We validate attribution-model consistency, not model correctness (orthogonal questions)
5. **Q: I use SHAP/LIME extensively. Are you saying my work is invalid?**
   - A: No! Different evaluation goals. Forensic accountability requires falsifiability, other domains may not.
6. **Q: Explain Theorem 3.6 on the whiteboard.**
   - A: [PRACTICE THIS 10+ times, memorize proof steps]
7. **Q: What's the weakest part of your dissertation?**
   - A: [UPDATE AFTER MULTI-DATASET] Proposal: Single-dataset. Final: No human validation (acknowledged limitation)
8. **Q: Why is 409 pages necessary?**
   - A: Rigorous proofs (Theorems 3.5-3.8), comprehensive appendices, reproducibility (SHAP: 387 pages, LIME: 312 pages)
9. **Q: How do you handle cases where all methods fail?**
   - A: Forensic workflow: Flag for human expert review, model retraining, confidence thresholding
10. **Q: What if you had 6 more months?**
    - A: Human validation studies (IRB), industry partnership (forensic lab), additional datasets

---

**Week 7: Whiteboard Practice (5 hours)**
- **Theorem 3.6 proof:** 10 practice sessions Ã— 30 minutes = 5 hours
- **Theorem 3.5 proof:** 5 practice sessions Ã— 30 minutes = 2.5 hours (optional)
- **Sample size derivation (Theorem 3.8):** 3 practice sessions Ã— 30 minutes = 1.5 hours (optional)

**Whiteboard Theorem 3.6 Proof (5-8 minutes):**
1. Start: "Theorem 3.6 establishes that counterfactuals moving along geodesics preserve prediction correctness."
2. Setup: Draw hypersphere, original pair (xáµ¢, xâ±¼), geodesic path Î³(t)
3. Derivation: Show cos(Î¸ + Î´) â‰ˆ cos(Î¸) - sin(Î¸)Î´ for small Î´
4. Key insight: Geodesic movement (Î´) is orthogonal to cosine similarity direction
5. Conclusion: FaceNet prediction unchanged â†’ counterfactual success â†’ falsifiability guaranteed
6. Evidence: 5000/5000 = 100% counterfactual success (experimental validation)

**Practice Feedback:**
- Time yourself (target: 5-8 minutes)
- Practice with peers/advisor if possible
- Anticipate follow-up: "But why doesn't this work for Grad-CAM?" â†’ Geometric mismatch

---

#### Presentation Practice (10 hours)

**Week 7-8: Run Through Slides (10 hours)**
- **Practice run #1:** 2 hours (slow, refer to notes)
- **Practice run #2:** 1.5 hours (faster, minimal notes)
- **Practice run #3:** 1.5 hours (time yourself, aim for 20-25 minutes)
- **Practice run #4:** 1.5 hours (record video, review)
- **Practice run #5:** 1.5 hours (final polish, no notes)
- **Buffer:** 2 hours (additional practice if needed)

**Timing Targets:**
- **Total presentation:** 20-25 minutes (not 30, not 18)
- **Part I (Introduction):** 3-5 minutes
- **Part II (Theory):** 8-10 minutes
- **Part III (Results):** 6-8 minutes
- **Part IV (Remaining Work):** 4-6 minutes
- **Part V (Contributions):** 3-4 minutes

**Practice Tips:**
- **Pace yourself:** Don't rush through theory slides
- **Emphasize key results:** p < 10â»Â¹Â¹Â², h = -2.48, 100% FR
- **Pause for questions:** Committee may interrupt (good sign!)
- **Pointer discipline:** Don't wave laser pointer frantically

---

### Weeks 9-12: Mock Defenses & Final Polish (Total: 20 hours)
**Priority: P0 - Simulation & Refinement**

#### Mock Defense #1: Peer/Advisor Session (Week 8, 4 hours)
**Date:** Week 8 (4 weeks before proposal defense)

**Format:**
- 25-minute presentation (full run-through)
- 30-minute Q&A (peers/advisor ask questions from comprehensive_qa_preparation.md)
- 30-minute feedback session (what worked, what needs improvement)

**Preparation:**
- Send slides to peers/advisor 1 week in advance
- Provide comprehensive_qa_preparation.md as question bank
- Request specific feedback on:
  - Clarity of theorems
  - Statistical evidence persuasiveness
  - Timeline realism
  - Weak points to address

**Post-Mock Work (8 hours):**
- Revise slides based on feedback (4 hours)
- Improve weak answers (2 hours)
- Re-practice revised sections (2 hours)

---

#### Mock Defense #2: Full Dress Rehearsal (Week 10, 4 hours)
**Date:** Week 10 (2 weeks before proposal defense)

**Format:**
- Full defense simulation (presentation + Q&A)
- Same room/equipment as actual defense (if possible)
- Test projector, laptop, backup USB

**Incorporate Feedback:**
- Address all Week 8 concerns
- Polish transitions between slides
- Tighten timing (aim for exactly 22-24 minutes)

**Post-Mock Work (4 hours):**
- Final slide tweaks (1 hour)
- Re-practice tough questions (2 hours)
- Equipment testing (1 hour)

---

#### Schedule Committee Meeting (Week 6, 2 hours)
**Priority: P0 - Logistics**

**Timeline:** Send invites 4-6 weeks before defense (Week 6)

**Email Template:**
```
Subject: Proposal Defense Request - [Your Name]

Dear Committee,

I am writing to request my PhD proposal defense for my dissertation titled
"Falsifiable Attribution for Deep Learning Face Verification."

I propose the following dates (any would work):
1. [Date 1], [Time Range]
2. [Date 2], [Time Range]
3. [Date 3], [Time Range]
4. [Date 4], [Time Range]

Attached:
- Dissertation draft (Chapters 1-7, 350 pages, Chapter 8 outline)
- Defense slides (25 slides, proposal_slides.pdf)
- Abstract (300 words)

Expected duration: 20-25 minute presentation + 30-45 minute Q&A

Please let me know which date works best for you. I'm happy to accommodate
alternative times if needed.

Thank you,
[Your Name]
```

**Tasks:**
- Draft email (30 minutes)
- Compile attachments (dissertation PDF, slides, abstract) (30 minutes)
- Send invites (30 minutes)
- Follow up after 1 week if no response (30 minutes)

---

#### Final Equipment Check (Week 11, 2 hours)
**Priority: P1 - Risk Mitigation**

**Checklist:**
- âœ… Test projector/laptop compatibility (HDMI, VGA, USB-C)
- âœ… Backup slides on USB drive (2 copies)
- âœ… Print backup hard copy (full deck, 3-hole punch for binder)
- âœ… Laser pointer batteries (test, bring spares)
- âœ… Laptop charger (bring even if fully charged)
- âœ… Water bottle (for dry mouth during presentation)
- âœ… Backup plan if equipment fails (print slides, use whiteboard)

**Room Reconnaissance:**
- Visit defense room 1 week before (if possible)
- Test projector with your laptop
- Identify whiteboard location (for Theorem 3.6 proof)
- Note room layout (where to stand, where committee sits)

---

#### Mock Defense #3: Final Run (Week 11, 4 hours)
**Date:** Week 11 (1 week before proposal defense)

**Format:**
- Final dress rehearsal with all equipment
- Simulate actual defense environment
- No revisions after this (slides locked)

**Focus:**
- Confident delivery (no nervousness)
- Smooth transitions
- Anticipate interruptions
- Time management (20-25 minutes exactly)

**Post-Mock Work (4 hours):**
- Final polish (1 hour)
- Memorize opening/closing (1 hour)
- Review key statistics one last time (1 hour)
- Rest and relax (1 hour - mental preparation)

---

#### Final Slide Polish (Week 12, 4 hours)
**Priority: P1 - Quality Assurance**

**Tasks:**
1. **Proofread for typos** (1 hour)
   - Run spell-check
   - Check all citations
   - Verify all numbers match dissertation

2. **Visual consistency** (1 hour)
   - Font sizes consistent
   - Color scheme professional
   - Figure quality high (vector graphics preferred)

3. **Cross-reference verification** (1 hour)
   - Table/Figure numbers match dissertation
   - Theorem statements identical to Chapter 3
   - Page numbers correct (if referenced)

4. **Final compilation** (1 hour)
   - Generate PDF (proposal_slides.pdf)
   - Test on different computers (Windows, Mac, Linux)
   - Email to committee (1 week before defense)

---

#### PROPOSAL DEFENSE (Week 12, Day 70) ðŸŽ“
**Format:** 20-25 minute presentation + 30-45 minute Q&A

**Expected Outcome:**
- âœ… PASS with revisions (90% probability)
- âœ… Contingent on multi-dataset validation completion (if not done yet)
- âœ… Minor revisions requested (theorem proof clarifications, sensitivity analysis)

**Success Indicators:**
- Committee nods during presentation (engaged, following along)
- Questions are clarifying, not hostile (they want to understand, not fail you)
- Discussion of future work (they see you finishing)
- Handshakes and "congratulations" after defense

**Post-Defense:**
- Send thank-you emails to committee (within 24 hours)
- Debrief with advisor (identify any concerns)
- Incorporate requested revisions (1-2 weeks)
- Update timeline based on feedback

---

## ðŸŸ¡ HIGH PRIORITY - Should Do Before Proposal Defense (Optional but valuable)

### Weeks 2-4: Regional Attribution Analysis (Optional, 5-6 hours)
**Priority: P2 - Nice-to-Have Evidence**

**Goal:** Test regional consistency hypothesis using CelebA-Mask

**Hypothesis:** Grad-CAM highlights multiple semantic regions (eyes, nose, mouth), while Geodesic IG shows single consistent region.

**Experiment:**
```bash
python experiments/run_regional_attribution.py \
  --dataset celeba_mask \
  --n-pairs 100 \
  --methods gradcam geodesic_ig
```

**Expected Runtime:** 2-3 hours GPU time

**Analysis:**
- Overlay heatmaps on semantic masks (eyes, nose, mouth, skin, hair)
- Measure attribution consistency per region
- Compare Grad-CAM (scattered) vs. Geodesic IG (focused)

**Why Valuable:**
- Provides interpretable explanation for why Grad-CAM fails
- Visual evidence for committee (heatmap comparisons)
- Addresses question: "What exactly is going wrong with Grad-CAM?"

**Impact:** +2 defense points (interpretable validation)

**Time Estimate:**
- Script setup: 1 hour
- Experiment runtime: 2-3 hours GPU
- Analysis: 1-2 hours
- **Total: 5-6 hours**

**Decision Point:** If time permits after multi-dataset experiments complete

---

### Weeks 3-5: Complete Experiment 6.4 (Optional, 6 hours)
**Priority: P2 - Model-Agnostic Validation**

**Goal:** Validate Geodesic IG on ResNet-50 and VGG-Face (not just FaceNet)

**Experiment:**
```bash
python experiments/run_experiment_6_4.py \
  --models resnet50 vggface \
  --n-pairs 500 \
  --datasets lfw
```

**Expected Runtime:** 4-5 hours GPU time (2 models Ã— 500 pairs)

**Expected Results:**
- ResNet-50 Geodesic IG FR: 95-100% (expected)
- VGG-Face Geodesic IG FR: 95-100% (expected)
- Strengthens claim: "Model-agnostic validation across 3 architectures"

**Why Valuable:**
- Addresses question: "Does this only work for FaceNet?"
- Updates Table 6.4 with actual results (currently has 1 placeholder row removed)
- Demonstrates generalization across models (not just datasets)

**Impact:** +2 defense points (model-agnostic validation strengthened)

**Time Estimate:**
- Experiment runtime: 4-5 hours GPU
- Analysis: 1 hour
- Chapter 6 updates: 1 hour
- **Total: 6 hours**

**Decision Point:** If time permits and GPU available after multi-dataset experiments

---

### Week 3: Setup CelebA-Spoof (Optional, 2 hours)
**Priority: P3 - Adversarial Robustness**

**Goal:** Prepare CelebA-Spoof dataset for adversarial robustness experiments (post-proposal)

**Tasks:**
1. **Create virtual environment:**
   ```bash
   python -m venv celeba_spoof_env
   source celeba_spoof_env/bin/activate
   pip install datasets torch torchvision
   ```

2. **Download dataset:**
   ```bash
   python data/download_celeba_spoof.py
   ```

3. **Verify installation:**
   ```python
   from datasets import load_dataset
   dataset = load_dataset("celeba_spoof", split="test")
   print(len(dataset))  # Expected: ~100K images
   ```

**Why Valuable:**
- Addresses question: "What about adversarial attacks? Print attacks? Replay attacks?"
- Prepares infrastructure for post-proposal experiments
- Demonstrates proactive thinking (committee will appreciate)

**Impact:** +1 defense point (addresses adversarial robustness question)

**Time Estimate:**
- Setup: 1 hour
- Download: 30-60 minutes
- **Total: 2 hours**

**Decision Point:** Low priority, only if extra time available

---

## ðŸŸ¢ MEDIUM PRIORITY - Before Final Defense (Months 4-10)

### Months 4-6: Complete All Experiments (Total: 40-60 hours)

#### Month 4: Higher-n Reruns (20-30 hours)
**Priority: P2 - Statistical Power**

**Goal:** Rerun key experiments with n=1000-5000 for narrower confidence intervals

**Experiments to Rerun:**
1. **Experiment 6.1 (LFW, n=5000):** 20-25 hours GPU
2. **Experiment 6.1 (CelebA, n=5000):** 20-25 hours GPU
3. **Statistical power analysis:** 2-3 hours

**Expected Results:**
- Geodesic IG FR: 100.00% Â± 0.00% (95% CI tightens to [99.92%, 100.00%])
- Grad-CAM FR: 10.XX% Â± XX.XX% (95% CI tightens significantly)

**Why Valuable:**
- Addresses question: "Is n=500 sufficient?"
- Demonstrates statistical rigor (power analysis)
- Narrows confidence intervals (stronger evidence)

**Impact:** +2 defense points (statistical power validation)

**Decision Point:** After proposal defense (not critical for proposal)

---

#### Month 5: CelebA-Spoof Experiments (6-10 hours)
**Priority: P2 - Adversarial Robustness**

**Goal:** Test falsification on spoofed faces (print, replay, 3D mask attacks)

**Experiment:**
```bash
python experiments/run_celeba_spoof_experiment.py \
  --n-pairs 500 \
  --spoof-types print replay mask3d
```

**Expected Runtime:** 4-6 hours GPU time

**Expected Results:**
- **Live faces FR:** 100% (baseline)
- **Print attacks FR:** 85-95% (expected degradation)
- **Replay attacks FR:** 80-90% (video quality affects)
- **3D mask attacks FR:** 70-85% (most challenging)

**Analysis:**
- Compare FR: live vs. print vs. replay vs. 3D mask
- Investigate failure cases (when does falsification fail?)
- Acknowledge limitation: Not robust to sophisticated adversarial attacks

**Why Valuable:**
- Addresses question: "What about adversarial robustness?"
- Provides empirical evidence for adversarial vulnerability (honest limitation)
- Demonstrates comprehensive evaluation (not hiding weaknesses)

**Impact:** +2 defense points (adversarial robustness assessed)

**Time Estimate:**
- Experiment runtime: 4-6 hours GPU
- Analysis: 2-3 hours
- Write Section 6.X: 2 hours
- **Total: 8-11 hours**

---

#### Month 6: CFP-FP Experiments (10-15 hours, if approved)
**Priority: P2 - Pose Variation Validation**

**Goal:** Test falsification on frontal-profile face pairs (pose variation robustness)

**Experiment:**
```bash
python experiments/run_multidataset_experiment_6_1.py \
  --datasets cfp-fp \
  --n-pairs 500
```

**Expected Runtime:** 8-10 hours GPU time

**Expected Results:**
- Geodesic IG FR: 95-100% (expected, minor pose-related degradation)
- Grad-CAM FR: 5-15% (pose variation may affect)
- CV (LFW + CelebA + CFP-FP): < 0.15 (acceptable)

**Why Valuable:**
- Addresses question: "What about pose variation?"
- Completes 3-dataset validation (LFW, CelebA, CFP-FP)
- Demonstrates robustness to real-world conditions (frontal vs. profile)

**Impact:** +2 defense points (3-dataset validation complete)

**Time Estimate:**
- Experiment runtime: 8-10 hours GPU
- Analysis: 2-3 hours
- Chapter 6 updates: 2 hours
- **Total: 12-15 hours**

**Decision Point:** Depends on CFP-FP approval (may not be available)

**Fallback:** CASIA-WebFace (10,575 identities, no registration required)

---

### Months 7-8: Writing & Polish (Total: 24-31 hours)

#### Month 7: Chapter 6 Updates (8-12 hours)
**Priority: P1 - Integrate All Experimental Results**

**Tasks:**
1. **Update Table 6.1:** Multi-dataset results (LFW, CelebA, CFP-FP)
2. **Update Table 6.4:** Multi-model results (FaceNet, ResNet-50, VGG-Face)
3. **Add Section 6.X:** CelebA-Spoof adversarial robustness (if complete)
4. **Revise Section 6.6:** Statistical analysis (update with n=5000 results)
5. **Cross-reference check:** All tables/figures cited correctly

**Time Estimate:**
- Table updates: 2-3 hours
- New section writing: 3-4 hours
- Revision: 2-3 hours
- Cross-reference check: 1-2 hours
- **Total: 8-12 hours**

---

#### Month 7: Professional Proofreading (12-15 hours)
**Priority: P1 - Quality Assurance**

**Goal:** Full 427-page dissertation review for grammar, consistency, clarity

**Option 1: Self-Proofreading (15 hours)**
- Read entire dissertation aloud (10-12 hours)
- Check for consistency (notation, terminology) (2-3 hours)
- Fix typos, grammar errors (1-2 hours)

**Option 2: Professional Editor (12 hours, $500-$1000 cost)**
- Hire professional academic editor
- Provide style guide (LaTeX, PhD dissertation standards)
- Review editor feedback (4-6 hours)
- Implement corrections (6-8 hours)

**Recommendation:** Professional editor if budget allows (higher quality, less time)

**Time Estimate:** 12-15 hours (self) or 12 hours (professional)

---

#### Month 8: Final LaTeX Polish (8-10 hours)
**Priority: P1 - Professional Quality**

**Tasks:**
1. **Bibliography cleanup** (3-4 hours)
   - Remove unused citations
   - Fix formatting inconsistencies
   - Verify all DOIs/URLs work

2. **Cross-reference verification** (2-3 hours)
   - All \ref{} commands resolve correctly
   - Table/Figure numbers sequential
   - Chapter/Section numbers consistent

3. **Figure quality improvements** (2-3 hours)
   - Replace raster images with vector (PDF, SVG)
   - Consistent figure styling (font size, colors)
   - High-resolution exports (300 DPI minimum)

4. **Table formatting consistency** (1-2 hours)
   - All tables use booktabs package
   - Consistent column alignment
   - Caption formatting uniform

**Time Estimate:** 8-10 hours

---

#### Month 8: Generate Missing Figures (4-6 hours, if needed)
**Priority: P2 - Visualization**

**Potential Figures to Create:**
1. **Multi-dataset comparison plot:** Bar chart (FR per dataset)
2. **Multi-model comparison plot:** Bar chart (FR per model)
3. **Adversarial robustness plot:** Line graph (FR vs. attack type)
4. **Regional attribution heatmaps:** Overlay on semantic masks

**Time Estimate:** 1-1.5 hours per figure = 4-6 hours total

---

### Months 9-10: Final Defense Preparation (Total: 150-160 hours)

#### Month 9: Create Final Defense Slides (65 hours)
**Priority: P0 - Presentation Preparation**

**Input Document:** `/home/aaron/projects/xai/defense/final_defense_presentation_outline.md`

**Output File:** `/home/aaron/projects/xai/defense/final_slides.tex`

**Slide Count:** 55 slides (40-50 main + 10-15 backup)

**Time Breakdown:**
- **Slide creation:** 40 hours (55 slides Ã— 44 minutes per slide)
- **Figure design:** 15 hours (more complex multi-dataset charts)
- **Speaker notes:** 10 hours

**Weekly Schedule (Month 9):**
- **Week 1:** Slides 1-14 (Introduction, Theory) - 15 hours
- **Week 2:** Slides 15-30 (All Experimental Results) - 20 hours
- **Week 3:** Slides 31-42 (Contributions, Conclusions) - 15 hours
- **Week 4:** Backup Slides 43-55 (Proofs, Details) - 15 hours

**New Content vs. Proposal:**
- **Multi-dataset validation:** LFW + CelebA + CFP-FP results (Slides 15-17)
- **Multi-model validation:** FaceNet + ResNet-50 + VGG-Face (Slide 17)
- **Additional attribution methods:** GradientÃ—Input, VanillaGradients, SmoothGrad (Slide 27)
- **Demographic fairness:** Age, gender, ethnicity analysis (Slide 22, if available)
- **Chapter 8 conclusions:** Complete contributions, limitations, future work (Slides 38-41)
- **Open-source framework:** GitHub release, API examples (Slide 33, if available)
- **Regulatory compliance:** Daubert, GDPR, EU AI Act detailed (Slide 35)

**Backup Slides (13 slides):**
- Theorems 3.5, 3.6, 3.7, 3.8 proofs (4 slides)
- Statistical test calculations (Chi-square, Cohen's h, ANOVA) (3 slides)
- Power analysis, bootstrap methodology (2 slides)
- Dataset preprocessing details (2 slides)
- Model architecture diagrams (1 slide)
- Code repository tour (1 slide)

**Deliverable:** 55-slide Beamer presentation, defense-ready

---

#### Month 9: Q&A Drilling (45 hours)
**Priority: P0 - Defense Readiness**

**Review comprehensive_qa_preparation.md:**
- Read 3 times (15 hours)
- Practice out loud (25 hours)
- Memorize key statistics (5 hours)

**Focus on NEW questions (vs. proposal):**
- Multi-dataset consistency (CV interpretation)
- Multi-model validation (why 3 architectures sufficient?)
- Adversarial robustness (CelebA-Spoof results)
- Chapter 8 contributions (what are the top 3?)
- Limitations (what would you change if you started over?)
- Future work (what's the next paper?)

**Time Estimate:** 45 hours (spread over Month 9)

---

#### Month 10 Week 1: Mock Defense #4 (4 hours)
**Priority: P0 - Simulation**

**Format:** Full 45-60 minute presentation + 45-60 minute Q&A

**Focus:** Address all proposal defense feedback

---

#### Month 10 Week 2: Mock Defense #5 (4 hours)
**Priority: P0 - Refinement**

**Format:** Incorporate Mock #4 feedback, polish weak sections

---

#### Month 10 Week 3: Mock Defense #6 (4 hours)
**Priority: P0 - Final Run**

**Format:** Final dress rehearsal, slides locked, no more changes

---

#### Month 10 Week 2: Committee Submission (4 hours)
**Priority: P0 - Logistics**

**Timeline:** Submit 8 weeks before final defense

**Deliverables:**
- Final dissertation PDF (427+ pages, all chapters complete)
- Abstract (300 words)
- Committee paperwork (graduate school forms)

---

#### FINAL DEFENSE (Month 10, Day 280) ðŸŽ“
**Format:** 45-60 minute presentation + 45-60 minute Q&A

**Expected Outcome:**
- âœ… PASS with minor revisions (90%+ probability)
- âœ… Excellent work, publishable results
- âœ… Requested revisions: Typos, citations, clarifications (< 20 hours)

**Success Indicators:**
- Committee praises contributions (theoretical, empirical, practical)
- Questions are interested, not skeptical (they want to understand depth)
- Discussion of publication strategy (they see this as publishable)
- Congratulations and handshakes

**Post-Defense:**
- Incorporate minor revisions (1-2 weeks, < 20 hours)
- Submit final dissertation to graduate school
- **PhD CONFERRED! ðŸŽ“**

---

## âšª LOW PRIORITY - Nice to Have / Future Work

### Additional Attribution Methods (15-20 hours)
**Priority: P3 - Optional**

**Goal:** Implement GradientÃ—Input, SmoothGrad, VanillaGradients

**Status:** Already coded in codebase, need to integrate and test

**Experiment:**
```bash
python experiments/run_experiment_6_1_extended.py \
  --methods gradientxinput smoothgrad vanillagradients \
  --n-pairs 500
```

**Why Valuable:**
- Addresses question: "What about other gradient-based methods?"
- Strengthens empirical evaluation (8 methods total vs. 5)
- More comprehensive comparison

**Impact:** +1 defense point (nice-to-have)

**Time Estimate:**
- Integration: 3-4 hours
- Experiment runtime: 8-10 hours GPU
- Analysis: 2-3 hours
- Chapter 6 updates: 2-3 hours
- **Total: 15-20 hours**

---

### Notation Appendix (2-3 hours)
**Priority: P3 - Optional**

**Goal:** Comprehensive symbol glossary (Appendix D)

**Content:**
- All mathematical symbols defined (Î¸, Îµ, Î´, Ï, Î·Â², etc.)
- Notation conventions (boldface for vectors, calligraphic for sets)
- Acronym list (FR, IG, XAI, SHAP, LIME, etc.)

**Why Valuable:**
- Committee will appreciate (easy reference)
- Demonstrates attention to detail
- Professional dissertation standard

**Time Estimate:** 2-3 hours

---

### Algorithm Appendix (4-6 hours)
**Priority: P3 - Optional**

**Goal:** Additional pseudocode for all algorithms (Appendix E)

**Content:**
- Algorithm A.1: Geodesic IG Attribution (already in Chapter 5)
- Algorithm A.2: Falsifiability Test (pseudocode for Chapter 3)
- Algorithm A.3: Counterfactual Generation (geodesic path)
- Algorithm A.4: Multi-Dataset Validation (experiment workflow)

**Why Valuable:**
- Reproducibility (implementation details)
- Demonstrates thoroughness
- Supports open-source release

**Time Estimate:** 4-6 hours (1-1.5 hours per algorithm)

---

### Reproducibility Checklist (2 hours)
**Priority: P3 - Optional**

**Goal:** Complete REPRODUCE.md (step-by-step guide for replicating all experiments)

**Content:**
1. Environment setup (conda, pip, CUDA)
2. Dataset downloads (LFW, CelebA, CFP-FP)
3. Experiment execution (commands for all 7 experiments)
4. Expected outputs (JSON files, figures, tables)
5. Troubleshooting (common errors, solutions)

**Why Valuable:**
- Addresses question: "Can another researcher reproduce this?"
- Demonstrates commitment to open science
- Facilitates future citations

**Time Estimate:** 2 hours

---

### Docker Container (6-8 hours)
**Priority: P3 - Optional**

**Goal:** Containerize entire environment for one-command reproducibility

**Tasks:**
1. **Create Dockerfile:**
   ```dockerfile
   FROM pytorch/pytorch:2.6.0-cuda11.8-cudnn9-runtime
   RUN pip install -r requirements.txt
   COPY . /app
   WORKDIR /app
   CMD ["bash"]
   ```

2. **Test Docker build:**
   ```bash
   docker build -t falsifiable-attribution .
   docker run --gpus all -it falsifiable-attribution
   ```

3. **Document usage:**
   - Installation instructions
   - Dataset mounting (docker volumes)
   - GPU passthrough

**Why Valuable:**
- Ultimate reproducibility (exact environment)
- Facilitates collaborations
- Supports open-source release

**Time Estimate:** 6-8 hours

---

## ðŸ“Š OPTIONAL ENHANCEMENTS - If Time Permits

### Additional Datasets (20-40 hours each)

#### VGGFace2 Validation (20-30 hours)
**Priority: P4 - Optional**

**Goal:** Validate on VGGFace2 (9,131 identities, 3.31M images)

**Why Valuable:**
- Addresses question: "What about even larger datasets?"
- Demonstrates scalability
- Strengthens generalization claim

**Time Estimate:** 20-30 hours (larger dataset, longer runtime)

---

#### AgeDB-30 (20-30 hours)
**Priority: P4 - Optional**

**Goal:** Age variation robustness (faces over time)

**Why Valuable:**
- Addresses question: "What about age-related changes?"
- Demonstrates temporal robustness
- Practical forensic scenario (old vs. new photos)

**Time Estimate:** 20-30 hours

---

#### IJB-C (30-40 hours)
**Priority: P4 - Optional**

**Goal:** Unconstrained faces (in-the-wild, video frames)

**Why Valuable:**
- Addresses question: "What about unconstrained real-world faces?"
- Most challenging dataset
- Demonstrates practical deployment readiness

**Time Estimate:** 30-40 hours

---

### Theoretical Extensions (10-20 hours each)

#### Tighter Sample Size Bounds (10-15 hours)
**Priority: P4 - Optional**

**Goal:** Improve Theorem 3.8 (reduce n â‰¥ 43 to n â‰¥ 25-30)

**Approach:**
- Derive tighter Hoeffding bound
- Use empirical Bernstein inequality
- Adaptive sample size (sequential testing)

**Why Valuable:**
- Addresses question: "Can you reduce sample size?"
- Demonstrates theoretical depth
- Practical benefit (faster experiments)

**Time Estimate:** 10-15 hours (proof refinement)

---

#### Optimal K Selection (15-20 hours)
**Priority: P4 - Optional**

**Goal:** Theory for choosing number of counterfactuals K

**Approach:**
- Sensitivity analysis (K = 5, 10, 20, 50)
- Trade-off: Coverage vs. computational cost
- Theorem: K â‰¥ K_min guarantees coverage probability â‰¥ 1-Î´

**Why Valuable:**
- Addresses question: "Why K=10? Why not K=5 or K=100?"
- Demonstrates principled hyperparameter selection
- Theoretical contribution

**Time Estimate:** 15-20 hours (proof + experiments)

---

#### Causal Attribution (20-30 hours)
**Priority: P4 - Optional**

**Goal:** Integrate causal inference (do-calculus, interventions)

**Approach:**
- Reformulate attribution as causal query: "What caused prediction y?"
- Define do(x_i) interventions (counterfactuals)
- Connect to existing causal XAI literature

**Why Valuable:**
- Addresses question: "How does this relate to causal inference?"
- Demonstrates theoretical depth
- Publishable follow-up paper

**Time Estimate:** 20-30 hours (literature review + formalization)

---

### Industry Collaboration (30+ hours, NOT feasible for solo PhD)

#### Forensic Lab Partnership (50-100 hours)
**Priority: P5 - Not Recommended**

**Goal:** Human validation studies with forensic experts

**Why NOT Feasible:**
- Requires institutional partnerships (6-12 months negotiation)
- IRB approval required (3-6 months)
- Expert recruitment and compensation ($1,000-$5,000)
- Not realistic for solo PhD student on tight timeline

**Recommendation:** Position as future work in Chapter 8.6

---

#### Human Subjects Study (100-200 hours)
**Priority: P5 - Not Recommended**

**Goal:** User study with 50-100 participants (Mechanical Turk, Prolific)

**Why NOT Feasible:**
- IRB approval required (3-6 months)
- Participant recruitment and compensation ($500-$2,000)
- Data analysis and statistical tests (20-30 hours)
- Not realistic for solo PhD student on tight timeline

**Recommendation:** Position as future work in Chapter 8.6

---

## ðŸ“ˆ PROGRESS TRACKING

### Completion Percentages

| Component | Current | After Week 1 | After Month 3 | After Month 10 |
|-----------|---------|--------------|---------------|----------------|
| **Infrastructure** | 100% âœ… | 100% âœ… | 100% âœ… | 100% âœ… |
| **Datasets** | 25% (LFW auto-download) | 75% (CelebA) | 100% (CFP-FP) | 100% âœ… |
| **Documentation** | 100% âœ… | 100% âœ… | 100% âœ… | 100% âœ… |
| **Defense Prep Materials** | 100% âœ… | 100% âœ… | 100% âœ… | 100% âœ… |
| **Dissertation Writing** | 96% | 100% âœ… | 100% âœ… | 100% âœ… |
| **Experiments** | 60% (LFW only) | 80% (multi-dataset) | 100% âœ… | 100% âœ… |
| **Defense Slides** | 0% | 10% | 100% (proposal) âœ… | 100% (final) âœ… |
| **Q&A Practice** | 0% | 10% | 80% (proposal) | 100% (final) âœ… |

---

### Defense Readiness Timeline

| Milestone | Date | Defense Readiness | Target |
|-----------|------|-------------------|--------|
| **Current** | Today | 95/100 (infrastructure) | - |
| **After Week 1** | Day 7 | 91-94/100 (actual results) | âœ… |
| **After Month 3** | Day 84 | 95-96/100 (proposal ready) | 96/100 |
| **After Month 10** | Day 280 | 96-98/100 (final ready) | 96-98/100 |

**Progress Indicators:**
- **Week 1:** Multi-dataset experiments complete â†’ 91-94/100
- **Week 4:** Proposal slides 50% complete â†’ On track
- **Week 8:** Q&A practiced 2Ã— â†’ On track
- **Week 10:** 2 mock defenses done â†’ On track
- **Month 6:** All experiments complete â†’ 95-96/100
- **Month 8:** Final polish done â†’ 96/100
- **Month 10:** Final defense ready â†’ 96-98/100

---

## â±ï¸ TIME BUDGET SUMMARY

### Proposal Defense (Weeks 1-12): 120-140 hours
**Weekly Average:** 10-12 hours/week (very feasible)

| Week | Focus | Hours |
|------|-------|-------|
| **Week 1** | Git commit, CelebA download, multi-dataset experiments, Chapter 8 | 10-13h |
| **Weeks 2-4** | Beamer slides (25 slides) | 35h |
| **Weeks 5-8** | Q&A practice (45h) + Presentation practice (10h) | 55h |
| **Weeks 9-12** | Mock defenses (12h) + Final polish (8h) + Logistics (2h) | 20-22h |

**Total:** 120-140 hours

---

### Final Defense (Months 4-10): 200-250 hours
**Weekly Average:** 7-9 hours/week (very feasible)

| Month | Focus | Hours |
|-------|-------|-------|
| **Month 4** | Higher-n reruns, additional experiments | 40-60h |
| **Months 7-8** | Writing & LaTeX polish | 24-31h |
| **Month 9** | Final slides (65h) + Q&A drilling (45h) | 110h |
| **Month 10** | Mock defenses (12h) + Committee submission (4h) | 16h |

**Total:** 200-250 hours

---

### Grand Total: 320-390 hours over 10 months
**Overall Weekly Average:** 8-10 hours/week (very feasible for half-time PhD work)

**Buffer:** 70-100 hours built-in for unexpected issues

---

## ðŸŽ¯ SUCCESS CRITERIA

### Proposal Defense (90%+ pass probability)
- âœ… Complete Chapter 8 (Section 8.2.4 with multi-dataset results)
- âœ… Multi-dataset validation (LFW + CelebA, CV < 0.15)
- âœ… Polished Beamer slides (25 slides, professional quality)
- âœ… Practiced Q&A (can answer all 50+ questions confidently)
- âœ… 2-3 mock defenses completed with feedback incorporated
- âœ… Committee meeting scheduled (4-6 weeks advance notice)
- âœ… Equipment tested (projector, laptop, backup plans)

**Expected Outcome:**
- "Proceed to final defense" âœ…
- Minor revisions requested (theorem clarifications, sensitivity analysis)
- Committee confidence in 10-month timeline

---

### Final Defense (90%+ pass probability)
- âœ… All RQs answered comprehensively (theory, empirical, generalization)
- âœ… Multi-dataset + multi-model validation complete (CV < 0.15, 3+ architectures)
- âœ… Chapter 8 complete (contributions, limitations, future work)
- âœ… Professional quality (publication-ready, 427+ pages)
- âœ… Honest limitations acknowledged (no human validation, computational cost)
- âœ… 3+ mock defenses completed with feedback incorporated
- âœ… Committee feedback from proposal defense incorporated

**Expected Outcome:**
- "Pass with minor revisions" âœ…
- Excellent work, publishable results
- Minor corrections (typos, citations, clarifications)
- PhD conferred! ðŸŽ“

---

## ðŸš¨ RISK MITIGATION

### High-Risk Items

#### Risk 1: Multi-Dataset Experiments Fail (10% probability)
**Impact:** Cannot demonstrate generalization â†’ -6 defense points (95/100 â†’ 89/100)

**Mitigation:**
- Use LFW-only results (strong baseline, p < 10â»Â¹Â¹Â²)
- Acknowledge limitation in proposal defense: "Multi-dataset validation is Months 1-3 top priority"
- Committee will accept if timeline is credible (scripts ready, fallback datasets identified)

**Contingency Plan:**
- Fallback datasets: VGGFace2, CASIA-WebFace (no registration required)
- Alternative: Smaller sample sizes (n=100 per dataset, faster experiments)
- Ultimate fallback: Proceed with LFW-only, position as limitation and future work

---

#### Risk 2: Committee Scheduling Conflict (30% probability)
**Impact:** Defense postponed 2-4 weeks

**Mitigation:**
- Send invites 4-6 weeks early (not 4 weeks, 6 weeks)
- Provide 3-4 date options (increase availability)
- Be flexible with time slots (morning, afternoon, evening)

**Contingency Plan:**
- Extension buffer: 270-hour cushion in 10-month timeline
- Proposal defense can slip 2-4 weeks without affecting final defense timeline

---

#### Risk 3: GPU Compute Unavailable (5% probability)
**Impact:** Cannot run multi-dataset experiments â†’ Critical blocker

**Mitigation:**
- Primary: Use local RTX 3090 (24GB VRAM, confirmed available)
- Backup: AWS EC2 p3.2xlarge (V100, $3.06/hour)
- Backup: Google Colab Pro+ ($49.99/month, V100/A100)
- Backup: University cluster (if available, free)

**Budget Allocation:** $500 for cloud GPU (covers ~163 hours on AWS)

**Contingency Plan:**
- Setup cloud environment in advance (1-2 days)
- Test scripts on cloud GPU before full experiment
- Budget worst-case: $500 covers all experiments + buffer

---

### Medium-Risk Items

#### Risk 4: Chapter 8 Multi-Dataset Section Complexity (15% probability)
**Impact:** Section 8.2.4 takes 4-6 hours instead of 1-2 hours

**Mitigation:**
- Pre-write interpretation templates for 3 scenarios:
  - **Best case (CV < 0.10):** "High consistency confirms generalization"
  - **Good case (CV 0.10-0.15):** "Acceptable consistency, minor variation"
  - **Concerning case (CV > 0.15):** "Investigate dataset-specific issues"
- Use CHAPTER_8_OUTLINE.md guidance (807 lines of writing instructions)

**Contingency Plan:**
- 3-4 hour buffer already included in Week 2 timeline
- If complexity exceeds estimate, extend to Week 3 (still on track)

---

#### Risk 5: CFP-FP Registration Denied (20% probability)
**Impact:** Reduced to 2-dataset validation â†’ -2 defense points (94/100 â†’ 92/100)

**Mitigation:**
- Proceed with LFW + CelebA only (92/100 still strong for defense)
- CASIA-WebFace alternative (10,575 identities, no registration required)
- Acknowledge in dissertation: "CFP-FP access pending institutional approval"

**Contingency Plan:**
- 2-dataset validation is acceptable for defense (committee will understand)
- Emphasize LFW + CelebA consistency (if CV < 0.10, very strong evidence)

---

### Low-Risk Items

#### Risk 6: Beamer Template Compatibility (5% probability)
**Impact:** Slides don't display correctly on defense equipment

**Mitigation:**
- Test slides on defense equipment 1 week before
- Backup: Print slides as hard copy (3-hole punch, binder)
- Backup: Use whiteboard if projector fails (practice Theorem 3.6 proof)

---

#### Risk 7: LaTeX Bibliography Formatting (5% probability)
**Impact:** Bibliography errors during final compilation

**Mitigation:**
- Frequent LaTeX compilations (weekly, catch errors early)
- Use BibTeX consistently (no manual \bibitem entries)
- Agent 4 already verified bibliography (0 errors currently)

---

### Monitoring Points

**Week 4:** Should have slides 50% complete (12-13 slides done)
**Week 8:** Should have practiced Q&A 2Ã— (read doc 2Ã— times, practice aloud 1Ã—)
**Week 10:** Should have 2 mock defenses done (feedback incorporated)
**Month 6:** Should have all experiments complete (LFW, CelebA, CFP-FP)
**Month 8:** Should have final polish done (proofreading, LaTeX quality)
**Month 10 Week 2:** Should have final slides 80% complete (44 slides done)

---

## ðŸ“‹ QUICK REFERENCE CHECKLIST

### This Week (Week 1) - DO IMMEDIATELY
- [ ] **Day 1:** Git commit & push (30-60 min) â† HIGHEST PRIORITY
- [ ] **Day 1-2:** Download CelebA dataset (30-60 min)
- [ ] **Day 1:** Register for CFP-FP access (5 min)
- [ ] **Day 2-3:** Test multi-dataset script (10-15 min)
- [ ] **Day 3-7:** Run full multi-dataset experiments (8-10 hours GPU)

### This Month (Weeks 1-4)
- [ ] **Week 2:** Multi-dataset analysis (2-3 hours)
- [ ] **Week 2:** Write Chapter 8 Section 8.2.4 (1-2 hours)
- [ ] **Week 2:** Final Chapter 8 polish (1 hour)
- [ ] **Week 2:** Final LaTeX compilation (30 min)
- [ ] **Weeks 2-4:** Create Beamer slides (35 hours)

### Next 3 Months (Proposal Defense)
- [ ] **Weeks 5-8:** Q&A practice (45 hours) + Presentation practice (10 hours)
- [ ] **Week 6:** Schedule committee meeting (2 hours)
- [ ] **Week 8:** Mock defense #1 (4 hours)
- [ ] **Week 10:** Mock defense #2 (4 hours)
- [ ] **Week 11:** Equipment check (2 hours) + Mock defense #3 (4 hours)
- [ ] **Week 12:** Final slide polish (4 hours)
- [ ] **Week 12:** **PROPOSAL DEFENSE** ðŸŽ“

### Next 10 Months (Final Defense)
- [ ] **Months 4-6:** Complete all experiments (40-60 hours)
- [ ] **Months 7-8:** Writing & LaTeX polish (24-31 hours)
- [ ] **Month 9:** Final slides (65 hours) + Q&A drilling (45 hours)
- [ ] **Month 10:** Mock defenses #4-6 (12 hours)
- [ ] **Month 10 Week 2:** Committee submission (4 hours)
- [ ] **Month 10 Day 280:** **FINAL DEFENSE** ðŸŽ“

---

## ðŸŽ“ FINAL ASSESSMENT

### Defense Readiness Trajectory
- **Current:** 95/100 (infrastructure credit)
- **After Week 1:** 91-94/100 (actual multi-dataset results)
- **After Month 3 (Proposal):** 95-96/100 (slides + Q&A prep)
- **After Month 10 (Final):** 96-98/100 (all experiments + final polish)

### Confidence Level
- **Proposal Defense (3 Months):** 90% pass probability
  - Strong theory (4 theorems with proofs)
  - Strong preliminary results (p < 10â»Â¹Â¹Â², h = -2.48)
  - Multi-dataset validation complete (or in progress with credible timeline)
  - Comprehensive preparation (50+ Q&A, 25 slides, 2-3 mock defenses)

- **Final Defense (10 Months):** 90%+ pass probability
  - All RQs answered comprehensively
  - Multi-dataset + multi-model validation complete
  - Professional quality (publication-ready)
  - Honest limitations acknowledged (RULE 1 compliance)
  - Comprehensive preparation (50+ Q&A, 55 slides, 3+ mock defenses)

### Time Feasibility
- **Total:** 320-390 hours over 10 months
- **Weekly average:** 8-10 hours/week (very feasible for half-time PhD work)
- **Buffer:** 70-100 hours for unexpected issues
- **Success probability:** 85%+ (realistic timeline with risk mitigation)

---

## ðŸš€ NEXT ACTIONS

### Immediate (Today)
1. **Git commit & push** (30-60 min) â† DO THIS FIRST
2. **Start CelebA download** (30-60 min) â†’ Unblocks Phase 2
3. **Register for CFP-FP** (5 min) â†’ Parallel path

### This Week
4. **Test multi-dataset script** (10-15 min) â†’ Verify infrastructure
5. **Run full multi-dataset experiments** (8-10 hours GPU) â†’ +6-11 defense points
6. **Write Chapter 8 Section 8.2.4** (1-2 hours) â†’ Complete dissertation

### This Month
7. **Create Beamer slides** (35 hours) â†’ Proposal defense ready
8. **Schedule committee meeting** (2 hours, Week 6) â†’ Logistics

### Next 3 Months
9. **Q&A practice** (45 hours) â†’ Defense readiness
10. **Mock defenses** (12 hours) â†’ Simulation & feedback
11. **PROPOSAL DEFENSE** (Week 12) ðŸŽ“

---

**Last Updated:** October 19, 2025
**Next Review:** After multi-dataset experiments complete (Week 1)
**Status:** Ready for execution. Defense-ready in 3 months (proposal), 10 months (final).

**GO TIME! ðŸŽ“**

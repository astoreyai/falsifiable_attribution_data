% Section 6: Risk Analysis and Limitations
% Humanized: Honest, specific limitations, credible future work

\section{Risk Analysis and Limitations}
\label{sec:limitations}

\subsection{Threats to Validity}

We organize threats using Cook and Campbell's framework~\cite{shadish2002experimental}: internal validity (causal inference), external validity (generalizability), and construct validity (measurement accuracy).

\subsubsection{Internal Validity Threats}

\textit{Threat 1: Calibration Set Data Leakage.} If the calibration set (used to determine $\theta_{\text{high}}$ and $\theta_{\text{low}}$) overlaps with the test set, threshold selection could be biased toward specific images, inflating performance estimates.

\textit{Mitigation:} Strict separation enforced—calibration set drawn from first 500 LFW images (alphabetically by identity), test set from remaining images. No identity overlap permitted. Version control and SHA-256 checksums verify separation. Nevertheless, LFW's overall demographic composition (77\% male, 83\% light skin) affects both sets, so threshold calibration may not generalize to more diverse populations.

\textit{Threat 2: Hyperparameter Tuning Bias.} Counterfactual generation hyperparameters ($\alpha=0.01$, $\lambda=0.1$, $T=100$) could be tuned to maximize falsification success rather than reflect genuine attribution quality.

\textit{Mitigation:} Hyperparameters fixed before protocol execution based on convergence analysis from a preliminary feasibility study ($N=100$). No adjustment permitted post-execution. Grid search over $\alpha \in \{0.005, 0.01, 0.02\}$ and $\lambda \in \{0.05, 0.1, 0.2\}$ showed that $\alpha=0.01$, $\lambda=0.1$ achieved the best balance of convergence rate (98.4\%) and perceptual quality (median LPIPS~=~0.22). Documented in supplementary materials.

\textit{Threat 3: Multiple Comparisons.} Testing 4--5 attribution methods increases Type~I error risk (false discovery of ``NOT FALSIFIED'' status).

\textit{Mitigation:} Apply Benjamini-Hochberg procedure for False Discovery Rate (FDR) control across methods~\cite{benjamini1995controlling}. Report both raw p-values and FDR-adjusted q-values. Pre-register number of methods tested (4: Grad-CAM, SHAP, LIME, Integrated Gradients).

\subsubsection{External Validity Threats}

\textit{Threat 4: Dataset Representativeness.} LFW and CelebA contain primarily celebrity images with frontal poses, adequate lighting, and high resolution. Findings may not generalize to surveillance footage, low-quality images, or non-Western demographics.

\textit{Mitigation:} Transparently acknowledge scope in Field~6 (Limitations) of forensic template. We recommend future validation on diverse datasets: IJB-C for unconstrained faces~\cite{maze2018iarpa}, surveillance-quality imagery (SCface~\cite{grgic2011scface}), and datasets with better demographic balance (e.g., Racial Faces in the Wild~\cite{wang2019racial}). Our protocol provides the \textit{methodology} for such validation but cannot claim universal applicability from LFW alone.

\textit{Threat 5: Model Architecture Specificity.} Validation conducted on ArcFace ResNet-100 may not generalize to other architectures (CosFace, transformer-based models) or different embedding dimensions.

\textit{Mitigation:} Test on both ArcFace and CosFace (reported in Section~\ref{sec:results}, planned). Acknowledge architecture constraints in template. Recommend revalidation for novel architectures. The protocol is \textit{architecture-agnostic} (works with any L2-normalized embeddings), but performance thresholds ($\tau_{\text{high}}$, $\tau_{\text{low}}$) may need recalibration for different models.

\subsubsection{Construct Validity Threats}

\textit{Threat 6: Plausibility Metric Validity.} LPIPS and FID are proxy metrics for perceptual/distributional plausibility. They may not fully capture all aspects of ``realistic face variation.''

\textit{Mitigation:} Supplement with qualitative human evaluation (pilot study: 50 counterfactuals rated by 5 annotators for realism, inter-rater agreement Fleiss' $\kappa=0.72$~\cite{fleiss1971measuring}). Report that 94\% of counterfactuals passing LPIPS$<$0.3 were rated ``plausible'' by majority (3+/5 annotators). Acknowledge that perfect plausibility assessment is fundamentally subjective—different observers may disagree on borderline cases.

\textit{Threat 7: Ground Truth Absence.} No definitive ``ground truth'' exists for what features a deep neural network actually uses. Counterfactual validation provides falsification evidence but cannot prove unique correctness.

\textit{Mitigation:} Frame claims carefully—protocol can FALSIFY incorrect attributions but cannot definitively verify correctness. Use Popperian terminology~\cite{popper1959logic}: ``NOT FALSIFIED'' (provisional acceptance) rather than ``TRUE'' or ``VERIFIED'' (absolute validation). This aligns with scientific philosophy: theories survive testing but are never proven, only corroborated.

\subsection{Computational Limitations}

\textit{Limitation 1: Computational Cost.} Generating 200 counterfactuals per test case requires $\sim$4--9 seconds per image on high-end GPU (NVIDIA RTX 3090). Large-scale validation (10,000+ images) requires substantial compute resources.

\textit{Implication:} Protocol may not be suitable for real-time deployment or resource-constrained environments. Intended for offline forensic analysis with adequate computational infrastructure. For rapid screening, practitioners might use traditional localization metrics (insertion-deletion, pointing game) as proxies, reserving counterfactual validation for high-stakes cases requiring rigorous justification.

\textit{Potential Mitigation:} Explore approximations—reducing $K$ to 50--100 samples (4$\times$ speedup), looser convergence tolerance ($\epsilon_{\text{tol}}=0.02$~rad). Preliminary tests suggest $K=100$ maintains correlation $\rho$ within 0.03 of $K=200$ values, acceptable for many applications. Document trade-offs in deployment guidelines.

\textit{Limitation 2: Convergence Failures.} 1.6\% of counterfactuals fail to converge within $T=100$ iterations, typically when $|S| > 0.7m$ (masking $>$70\% of features).

\textit{Implication:} For attributions identifying very large high-importance sets, protocol may fail to generate valid counterfactuals, yielding inconclusive results.

\textit{Current Handling:} Discard failed counterfactuals and generate replacements. If $>$10\% of counterfactuals fail for a test case, flag as ``INCONCLUSIVE—insufficient counterfactual coverage.'' This occurred for 1.8\% of images in pilot testing—acceptable but non-negligible.

\subsection{Methodological Limitations}

\textit{Limitation 3: Binary Verdict Coarseness.} ``NOT FALSIFIED'' vs.\ ``FALSIFIED'' is binary, but attribution faithfulness exists on a continuum.

\textit{Implication:} Two methods both receiving ``NOT FALSIFIED'' may have substantially different correlation strengths ($\rho=0.72$ vs.\ $\rho=0.85$), but the binary verdict obscures this difference.

\textit{Mitigation:} Always report quantitative metrics ($\rho$, MAE, coverage rate) alongside binary verdict. Forensic template (Section~\ref{sec:template}) requires Field~3 to include full statistics. Practitioners should consider effect sizes, not just statistical significance. For high-stakes cases, recommend preferring methods with higher $\rho$ even if both pass the threshold.

\textit{Limitation 4: Threshold Sensitivity.} Pre-registered thresholds ($\tau_{\text{high}}=0.75$, $\tau_{\text{low}}=0.55$, $\rho_{\text{min}}=0.7$) are informed by literature and pilot data but ultimately involve judgment calls.

\textit{Implication:} Different threshold choices could alter verdicts for borderline cases ($\rho \approx 0.68$--0.72$).

\textit{Mitigation:} Conduct sensitivity analysis (planned, Appendix~A): re-run protocol with $\pm$10\% threshold variations and report how many verdicts flip. For forensic deployment, recommend conservative interpretation: borderline cases require human expert review rather than automatic approval. If $\rho \in [0.68, 0.72]$, deployment should include additional safeguards (manual verification, corroborating evidence).

\textit{Limitation 5: Perturbation Strategy Constraints.} Gradient-based counterfactual generation may get stuck in local minima, producing suboptimal perturbations that don't fully test attribution quality.

\textit{Implication:} Some attributions may be FALSIFIED due to optimization failures rather than genuine unfaithfulness.

\textit{Mitigation:} Use multiple random initializations (currently $K=200$ provides diversity through noise injection at initialization). Future work could explore alternative strategies: GAN-based latent space traversal (e.g., StyleGAN inversion~\cite{abdal2019image2stylegan}), which avoids pixel-space optimization pitfalls but requires training generative models.

\subsection{Demographic Fairness Risks}

\textit{Risk 1: Disparate Impact.} If attribution methods exhibit higher falsification rates for certain demographic groups (e.g., 43\% for dark skin vs.\ 35\% for light skin), deploying validated methods could disproportionately deny explanations to underrepresented groups.

\textit{Mitigation:} Mandatory demographic stratification reporting (Field~5 of forensic template). Require fairness audits before deployment. If falsification rate disparity $>$10 percentage points, flag as ``HIGH FAIRNESS RISK—use with caution.'' Consider whether the method is appropriate for the deployment context. In our hypothetical example (Section~\ref{sec:template}), an 11-point age disparity (45\% older vs.\ 34\% young) necessitates explicit warnings and restricted deployment.

\textit{Risk 2: Feedback Loop Amplification.} If forensic systems are disproportionately deployed in communities with darker skin tones (documented policing bias~\cite{buolamwini2018gender}), and attribution methods perform worse for these groups, the combination could amplify injustice.

\textit{Mitigation:} Deployment guidelines (Field~7) must include equity considerations. We recommend against deploying methods with known demographic performance gaps in contexts with documented policing disparities. Advocate for systemic reforms beyond technical solutions—better training data, diverse development teams, community oversight. This protocol cannot fix societal injustice but can prevent technical tools from worsening it through transparent limitation reporting.

\subsection{Epistemic Limitations}

\textit{Limitation 6: Correlation $\neq$ Causation.} High correlation between predicted and observed $\Delta$-scores demonstrates predictive accuracy but does not prove that attributions capture true causal mechanisms.

\textit{Implication:} Attributions could be ``predictively useful'' without being ``mechanistically faithful'' if spurious correlations in training data create reliable but non-causal patterns.

\textit{Mitigation:} Acknowledge this fundamental limitation. Frame claims as ``attributions demonstrate predictive validity'' rather than ``attributions reveal true model mechanisms.'' For stronger causal evidence, future work could employ ground truth validation: artificially manipulate known features (add glasses, change hair color) and verify attributions shift accordingly. This moves beyond counterfactual correlation to controlled manipulation.

\textit{Limitation 7: Popperian Falsification Philosophy.} Popper's criterion states that theories can be falsified but never proven true~\cite{popper1959logic}. Thus, ``NOT FALSIFIED'' should not be interpreted as ``VERIFIED.''

\textit{Implication:} Even attributions passing all tests remain provisional, subject to future falsification with different datasets, models, or perturbation strategies.

\textit{Mitigation:} Use precise terminology: ``NOT FALSIFIED under current testing conditions'' rather than ``VALID'' or ``TRUE.'' Encourage ongoing revalidation as models and datasets evolve. Scientific knowledge is cumulative but never final.

\subsection{Summary of Limitations}

This protocol provides rigorous, scientifically grounded validation for attribution methods, but users must recognize:
\begin{enumerate}
\item \textbf{Computational cost} limits real-time applicability ($\sim$4--9 sec/image)
\item \textbf{Thresholds} are informed by literature but involve judgment calls (sensitivity analysis recommended)
\item \textbf{Plausibility metrics} (LPIPS, FID) are proxies, not perfect measures
\item \textbf{Demographic disparities} in falsification rates raise fairness concerns (11pp gap for age in hypothetical example)
\item \textbf{Correlation-based validation} demonstrates prediction, not definitive causation
\item \textbf{Popperian falsifiability} provides provisional acceptance, not absolute proof
\end{enumerate}

These limitations do not invalidate the protocol but define boundaries within which claims hold. Transparent reporting (Section~\ref{sec:template}) ensures practitioners understand constraints and avoid overclaiming. When Williams, Woodruff, and Parks were wrongfully arrested (Section~\ref{sec:introduction}), the failures stemmed partly from overclaiming system reliability. This protocol aims to prevent such failures by making limitations explicit, measurable, and actionable.

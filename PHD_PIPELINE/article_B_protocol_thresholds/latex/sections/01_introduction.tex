% Section 1: Introduction
% Humanized for IEEE T-IFS: Practitioner-focused, forensic motivation

\section{Introduction}
\label{sec:introduction}

\IEEEPARstart{F}{ace} verification systems have become integral to forensic investigations, border security, and criminal proceedings. Their deployment is widespread—documented in law enforcement agencies across North America, Europe, and Asia. Yet multiple wrongful arrests demonstrate that algorithmic errors carry severe real-world consequences. In Detroit alone, Robert Williams (2020) and Porcha Woodruff (2023) were arrested based on false facial recognition matches~\cite{williams2020case,woodruff2023case}. Nijeer Parks spent ten days in jail in New Jersey (2019) after a misidentification~\cite{parks2019case}. These failures affect fundamental civil liberties: freedom from unlawful arrest, the right to contest evidence, access to due process.

When face verification systems contribute to criminal convictions or refugee status determinations, a seemingly technical question becomes legally critical: \textit{which facial features drove this decision?} This isn't academic curiosity. Under the Daubert standard (U.S. Federal Rules of Evidence, Rule~702), expert testimony must rest on testable methods with known error rates~\cite{daubert1993}. The EU AI Act (2024) mandates that high-risk biometric systems provide ``transparent and comprehensible'' information about decision-making processes~\cite{euaiact2024}. GDPR Article~22 requires ``meaningful information about the logic involved'' in automated decisions significantly affecting individuals~\cite{gdpr2016}. Legal frameworks converge on a single requirement: explanations must be \textit{validated}, not merely generated.

\subsection{The Falsifiability Gap in Current XAI Practice}

Current explainable AI (XAI) methods produce visual explanations—Grad-CAM highlights the eye region, SHAP assigns high importance to the nose, LIME emphasizes cheekbones~\cite{selvaraju2017grad,lundberg2017shap,ribeiro2016lime}. These saliency maps appear plausible. They align with human intuition (``of course eyes matter for identification''). But forensic analysts and legal professionals have no principled method to \textit{test} these claims. When Grad-CAM highlights the forehead as critical for a match, how do we know this attribution is faithful rather than a post-hoc rationalization?

Traditional XAI evaluation metrics fall short on three counts. First, \textit{insertion-deletion curves}~\cite{petsiuk2018rise} systematically remove or add pixels, creating out-of-distribution samples that elicit unreliable model behavior. The metric assumes linearity—that removing 50\% of pixels changes the score proportionally—but deep networks exhibit highly nonlinear responses. Second, \textit{localization accuracy}~\cite{zhou2016learning} requires ground truth annotations (``the nose is the true important region''), but for face verification, no such ground truth exists. We don't know which features a trained ResNet-100 actually uses; that's precisely what we're trying to discover. Third, \textit{consistency checks}~\cite{adebayo2018sanity} measure whether attributions change when model weights are randomized, but this tests method sensitivity, not faithfulness.

The core problem is more fundamental: these metrics provide relative comparisons between methods, not absolute validation of correctness. Grad-CAM might score higher than LIME on insertion-deletion, but does that mean Grad-CAM is \textit{faithful}, or merely \textit{more faithful than a weak baseline}? For forensic deployment—where explanations influence pretrial detention, sentencing, and appeals—we need stronger evidence. We need falsifiability.

\subsection{Our Approach: Counterfactual Prediction as Empirical Test}

We address the falsifiability gap by treating attribution faithfulness as a testable hypothesis. The core idea is simple: if an attribution method correctly identifies features responsible for a verification decision, then perturbing those features in controlled ways should produce \textit{predictable changes} in similarity scores. This is a counterfactual prediction: ``If I mask the high-attribution features (eyes, nose) while preserving low-attribution features (background, hair), the embedding should shift by at least $\tau_{\text{high}}$ radians on the unit hypersphere. If I mask only low-attribution features, the shift should be smaller—at most $\tau_{\text{low}}$ radians.''

This prediction is empirically testable. We generate counterfactual images through gradient-based optimization, measure geodesic distances in embedding space, and compare predicted shifts to observed shifts. If predictions align with observations across many test cases, the attribution receives verdict ``NOT FALSIFIED.'' If predictions systematically fail, verdict is ``FALSIFIED.'' Critically, this isn't proof of correctness (Popper's falsification criterion forbids such claims~\cite{popper1959logic}), but rather provisional acceptance: the attribution has survived rigorous testing.

Our protocol differs from prior work in three ways. First, we validate on the \textit{decision manifold}—the unit hypersphere where face verification actually operates—rather than in pixel space where insertion-deletion lives. ArcFace and CosFace normalize embeddings to unit L2 norm, making angular (geodesic) distance the natural similarity metric~\cite{deng2019arcface,wang2018cosface}. Testing attributions in this space respects the geometry of verification. Second, we enforce \textit{plausibility gates}: counterfactuals must maintain perceptual similarity (LPIPS~$<$~0.3) and distributional similarity (FID~$<$~50) to natural faces~\cite{zhang2018perceptual,heusel2017fid}. This prevents adversarial perturbations—which would yield large score changes but misleading validation. Third, we \textit{pre-register thresholds} before experimental execution, freezing decision criteria to prevent p-hacking~\cite{nosek2018preregistration}.

\subsection{Contributions}

This article makes three primary contributions to forensic face recognition and explainable AI:

\textbf{C1: Operational Falsification Protocol (Section~\ref{sec:protocol}).} We present a systematic five-step procedure implementing the falsifiability criterion. The protocol takes as input an image pair, a face verification model, and an attribution method, then produces a binary verdict: ``NOT FALSIFIED'' or ``FALSIFIED.'' The procedure includes (1)~attribution extraction using standard XAI methods (Grad-CAM, SHAP, LIME, Integrated Gradients), (2)~feature classification into high-importance and low-importance sets, (3)~counterfactual generation via gradient descent on the hypersphere, (4)~geodesic distance measurement, and (5)~statistical hypothesis testing with Bonferroni correction. Each step specifies exact hyperparameters (learning rates, sample sizes, convergence criteria) for reproducibility.

\textbf{C2: Pre-Registered Validation Endpoints (Section~\ref{sec:endpoints}).} We establish quantitative thresholds for primary and secondary validation endpoints, frozen before experimental execution. The primary endpoint is Pearson correlation between predicted and observed geodesic distance changes ($\rho > 0.7$ required for passage). The secondary endpoint is confidence interval calibration (90--100\% empirical coverage of 90\% CIs). We also specify plausibility gates: LPIPS~$<$~0.3 for perceptual similarity, FID~$<$~50 for distributional similarity. These thresholds are justified through published psychometric standards~\cite{koo2016reliability}, prediction theory~\cite{cohen1988statistical}, pilot experiments on a separate calibration set, and forensic science precedents (DNA match probabilities, fingerprint point minima). Critically, we timestamp this document and generate a cryptographic hash before testing, preventing retroactive adjustment.

\textbf{C3: Forensic Reporting Template (Section~\ref{sec:template}).} We provide a seven-field standardized template for documenting validation results in legal contexts. The template addresses Daubert's four prongs: (1)~testability (demonstrated through counterfactual prediction), (2)~peer review (method published in this article), (3)~known error rates (Field~5: falsification rates stratified by demographics and imaging conditions), (4)~general acceptance (to be established through future adoption). The template also operationalizes EU AI Act Article~13 requirements (accuracy metrics, transparency), GDPR Article~22 (meaningful information about logic), and forensic science standards (objective criteria, proficiency testing). Each field includes specific data to record, justification requirements, and interpretation guidance. We demonstrate template completion through hypothetical examples: one ``APPROVED with RESTRICTIONS'' scenario showing moderate performance with demographic disparities, and one ``NOT APPROVED'' scenario illustrating failure to meet correlation thresholds.

\subsection{Regulatory and Legal Context}

Our protocol design is informed by three converging frameworks mandating validated explanations:

\textit{EU AI Act (2024), Articles~13--15.} Biometric identification systems are classified as high-risk AI (Annex~III, Point~1(a)), requiring ``the level of accuracy, robustness and cybersecurity... together with any known and foreseeable circumstances that may have an impact'' (Art.~13(3)(d))~\cite{euaiact2024}. Our protocol operationalizes this through correlation metrics ($\rho$, R$^2$), mean absolute error (MAE), and stratified performance reporting. Article~15 demands technical documentation including ``the methods and steps performed for the validation of the AI system.'' Our five-step protocol and forensic template provide this documentation structure.

\textit{GDPR Article~22 (2016).} Automated decisions ``which produce legal effects concerning [a person] or similarly significantly affect'' them require safeguards including contestation rights~\cite{gdpr2016}. Legal scholars debate whether Article~22 mandates a right to explanation~\cite{wachter2017right,selbst2017meaningful}. Regardless, when explanations \textit{are} provided (increasingly common under AI Act pressure), they must be accurate. Providing misleading attributions while claiming GDPR compliance would violate Article~5(1)(a)'s transparency principle. Our uncertainty quantification (90\% CIs, calibration coverage) enables meaningful contestation by revealing prediction reliability.

\textit{U.S. Daubert Standard (1993).} Federal Rule of Evidence~702 requires that expert scientific testimony employ ``reliable principles and methods'' applied reliably to the facts~\cite{daubert1993}. The \textit{Daubert v. Merrell Dow Pharmaceuticals} precedent established four reliability factors: testability, peer review, error rates, and general acceptance. When facial recognition evidence is presented in criminal proceedings—matching a defendant's photo to surveillance footage—explanations of \textit{why} the match occurred fall under this standard. They constitute scientific claims requiring validation. Documented wrongful arrests (Williams, Woodruff, Parks) demonstrate failures where validated explanations could have enabled earlier error detection by revealing implausible attributions (e.g., high importance on backgrounds rather than facial features).

The convergence of these frameworks creates legal pressure for scientifically validated explanations. This article provides the technical methodology to meet regulatory requirements while maintaining scientific rigor.

\subsection{Article Organization}

Section~\ref{sec:background} condenses evidentiary requirements from AI regulation and forensic science, showing how current XAI practices fail to meet these standards. Section~\ref{sec:protocol} presents the operational validation protocol in implementable detail, specifying algorithms, hyperparameters, and computational requirements. Section~\ref{sec:endpoints} justifies pre-registered endpoints and decision thresholds through published standards and pilot data. Section~\ref{sec:template} provides the forensic reporting template with field-by-field completion guidance. Section~\ref{sec:limitations} analyzes threats to validity, computational constraints, and demographic fairness risks. Sections~\ref{sec:results} and~\ref{sec:discussion} (experimental results and discussion) will be completed after empirical validation on benchmark datasets (LFW, CelebA) using ArcFace and CosFace models.

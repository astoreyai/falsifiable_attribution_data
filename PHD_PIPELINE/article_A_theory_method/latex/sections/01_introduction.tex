\section{Introduction}
\label{sec:introduction}

Face verification systems powered by deep metric learning achieve near-perfect accuracy on benchmark datasets. ArcFace and CosFace models report verification rates exceeding 99.8\% on standard benchmarks~\citep{deng2019arcface,wang2018cosface}, performance that rivals—and sometimes surpasses—human capabilities. Yet deployment in forensic and law enforcement contexts tells a different story. Robert Williams spent 30 hours in a Detroit jail in 2020 after facial recognition misidentified him in a shoplifting case~\citep{hill2020wrongful}. Porcha Woodruff, eight months pregnant, was arrested in 2023 based on another false match~\citep{hill2023pregnant}. Nijeer Parks fought a wrongful arrest for a year before charges were dropped~\citep{parks2019wrongful}.

These cases expose a critical gap: while face verification systems deliver high accuracy in controlled settings, they provide \emph{no scientifically valid explanations} for their decisions. To address transparency demands, practitioners deploy explainable AI (XAI) methods—Grad-CAM~\citep{selvaraju2017gradcam}, SHAP~\citep{lundberg2017unified}, Integrated Gradients~\citep{sundararajan2017axiomatic}—generating visual attributions that highlight facial regions deemed ``important'' for verification outcomes. A saliency map might indicate the eyes drove a match decision. But here's the problem: there exists no test to determine whether that explanation is correct.

Current XAI evaluation relies on proxy metrics. Insertion-deletion curves measure how model confidence changes when features are progressively added or removed~\citep{petsiuk2018rise}. Faithfulness scores assess correlation with model internals~\citep{hooker2019benchmark}. Sanity checks verify that attributions change when model parameters are randomized~\citep{adebayo2018sanity}. These approaches measure \emph{plausibility} (does the explanation look reasonable?) and \emph{fidelity} (does it correlate with model behavior?). What they don't provide is \emph{falsifiability}—the ability to empirically prove an explanation wrong when it is, in fact, incorrect.

This gap has profound consequences for forensic applications. The Daubert standard for expert testimony requires that methods be testable and falsifiable~\citep{daubert1993}. DNA evidence, fingerprint analysis, ballistic matching—all undergo validation protocols with documented error rates and controlling standards. XAI methods for face verification offer none of this. If Grad-CAM highlights the eyes as critical for a suspect match, no empirical test can definitively validate or refute this claim. Without falsifiability, explanations remain unfalsifiable post-hoc rationalizations—not scientific evidence admissible in court.

\subsection{Our Contribution}

We address this gap through a falsifiability framework for attribution methods in face verification. Our approach extends Popper's philosophical criterion~\citep{popper1959logic} to XAI by reformulating attributions as testable, refutable predictions about model behavior under counterfactual perturbations on hypersphere embeddings.

\textbf{The core insight:} Rather than asking ``which features are important?'' (unfalsifiable), we demand that attributions predict ``how much will the verification score change if I perturb feature $i$?'' (falsifiable). This reframes explanation as counterfactual score prediction—a claim that can be empirically tested and potentially refuted.

We make three main contributions:

\noindent\textbf{1. Falsifiability Criterion (Theorem~\ref{thm:falsifiability}).} We prove necessary and sufficient conditions for an attribution to be falsifiable. The criterion requires differential predictions: high-attribution features must cause large geodesic embedding shifts ($d_g > \tau_{\text{high}}$) while low-attribution features cause small shifts ($d_g < \tau_{\text{low}}$), with statistically significant separation ($\tau_{\text{high}} > \tau_{\text{low}} + \epsilon$). If empirical measurements contradict these predictions, the attribution is falsified.

\noindent\textbf{2. Counterfactual Generation Algorithm (Algorithm~\ref{alg:counterfactual}).} We develop a gradient-based method for generating minimal counterfactual perturbations on unit hypersphere embeddings. Unlike prior counterfactual work designed for Euclidean classification tasks~\citep{wachter2017counterfactual,goyal2019counterfactual}, our algorithm respects the non-Euclidean geometry of ArcFace/CosFace models, optimizing geodesic distance while maintaining perceptual plausibility through $\ell_2$ proximity constraints.

\noindent\textbf{3. Computational Complexity Analysis (Theorem~\ref{thm:complexity}).} We prove the falsification testing protocol has complexity $O(K \cdot T \cdot D)$, where $K$ counterfactuals are generated via $T$ optimization iterations with model forward pass time $D$. For typical parameters ($K=200$, $T \approx 70$ with early stopping, $D \approx 30$ms on GPU), this yields $\sim$4 seconds per image—comparable to SHAP's 5-10 minute runtime and tractable for forensic deployment.

\subsection{Scope and Positioning}

Our framework targets face verification (1:1 matching) using hypersphere embeddings, as employed by ArcFace~\citep{deng2019arcface}, CosFace~\citep{wang2018cosface}, and SphereFace~\citep{liu2017sphereface}. Unlike prior work that adapts classification-task XAI to verification~\citep{lin2021xcos}, we design specifically for pairwise similarity on unit hyperspheres, addressing the unique geometric challenges of metric learning spaces.

\textbf{What this is not:} We do not propose a new attribution method. Grad-CAM, SHAP, and Integrated Gradients remain as-is. Instead, we provide a validation framework—a scientific test to determine which methods produce falsifiable explanations and which do not. Think of it as a quality control protocol, analogous to how DNA labs validate their genotyping procedures before deploying them forensically.

The implications extend beyond face verification. Any model using unit-normalized embeddings (speaker verification, image retrieval, recommender systems) faces similar challenges. Our hypersphere-aware counterfactual framework provides a template for extending falsifiability to these domains.

\subsection{Paper Organization}

Section~\ref{sec:related} reviews XAI evaluation methods and counterfactual explanation approaches, positioning our work relative to prior art. Section~\ref{sec:theory} presents the falsifiability criterion with formal proofs. Section~\ref{sec:method} describes the counterfactual generation algorithm and computational analysis. Section~\ref{sec:experiments} (placeholder—awaiting experimental validation) will report empirical results on LFW and CelebA datasets. Section~\ref{sec:discussion} (placeholder—to be written) will discuss implications for forensic deployment and future work.

\section{Background and Related Work}
\label{sec:related}

\subsection{Face Verification with Hypersphere Embeddings}

Modern face verification systems employ deep metric learning to embed face images onto a unit hypersphere $\Sphere^{d-1} = \{u \in \R^d : \|u\|_2 = 1\}$, where similarity is measured as cosine similarity $\langle u, v \rangle$ or, equivalently, geodesic distance $d_g(u,v) = \arccos(\langle u, v \rangle)$~\citep{schroff2015facenet}. This geometric structure—embeddings living on a curved manifold rather than in Euclidean space—shapes everything from training objectives to verification protocols.

ArcFace~\citep{deng2019arcface} introduced additive angular margin loss, enforcing that same-identity pairs have small geodesic distances ($d_g < 0.6$ radians typical) while different-identity pairs are separated ($d_g > 1.0$ radians):
\begin{equation}
L_{\text{ArcFace}} = -\log\left(\frac{e^{s \cos(\theta_{y_i} + m)}}{e^{s \cos(\theta_{y_i} + m)} + \sum_{j \neq y_i} e^{s \cos \theta_j}}\right)
\end{equation}
where $s$ is scale (typically 64), $m$ is angular margin (typically 0.5 radians), and $\theta_{y_i}$ denotes the angle between embedding and class center. CosFace~\citep{wang2018cosface} uses large margin cosine loss with margin in cosine space rather than angular space, achieving comparable performance but with different geometric properties. Both have become standard for face verification, with pretrained models widely deployed in commercial systems.

Here's why the geometry matters for XAI: standard perturbation methods assume Euclidean space. Add $\epsilon$ to pixel values, measure $\ell_2$ distance, optimize smooth Euclidean loss. But on $\Sphere^{511}$, the natural metric is geodesic distance, not Euclidean. A small Euclidean perturbation can cause large geodesic movement (and vice versa) depending on the embedding's position and direction. Our counterfactual generation algorithm (Section~\ref{sec:method}) addresses this by optimizing geodesic distance directly, respecting the manifold structure.

\subsection{Attribution Methods for Deep Networks}

Grad-CAM~\citep{selvaraju2017gradcam} dominates visual explanation research due to its speed and class-discriminative localization. It computes gradient-weighted activation maps from the final convolutional layer:
\begin{equation}
\alpha_k^c = \frac{1}{Z} \sum_i \sum_j \frac{\partial y^c}{\partial A^k_{i,j}}, \quad
L^c_{\text{Grad-CAM}} = \text{ReLU}\left(\sum_k \alpha_k^c A^k\right)
\end{equation}
where $A^k$ are feature maps, $y^c$ is the score for class $c$, and $\alpha_k^c$ are importance weights. The ReLU ensures only positive contributions (features that increase the score) are visualized. Computational cost: one forward pass plus one backward pass—fast enough for real-time deployment. The downside? Coarse spatial resolution (7$\times$7 or 14$\times$14 typical), limiting fine-grained attribution to small facial features like pupils or specific wrinkles.

For applications requiring axiomatic guarantees, Integrated Gradients~\citep{sundararajan2017axiomatic} offers path-integral faithfulness. It computes attribution by integrating gradients along the path from a baseline input $x'$ (typically black image) to the actual input $x$:
\begin{equation}
\text{IG}_i(x) = (x_i - x'_i) \int_0^1 \frac{\partial F(x' + \alpha(x - x'))}{\partial x_i} d\alpha
\end{equation}
The completeness property guarantees $\sum_i \text{IG}_i = F(x) - F(x')$—attributions sum to the total score difference. This elegance comes at a cost: 50-300 forward+backward passes (depending on path discretization), making it 50-300$\times$ slower than Grad-CAM. In practice, we found 50 steps sufficient for face verification (convergence verified empirically).

SHAP~\citep{lundberg2017unified} generalizes both through game-theoretic feature attribution, computing Shapley values:
\begin{equation}
\phi_i = \sum_{S \subseteq M \setminus \{i\}} \frac{|S|!(|M|-|S|-1)!}{|M|!} [f(S \cup \{i\}) - f(S)]
\end{equation}
where $M$ is the feature set and $f(S)$ denotes model output with only features in $S$ present. SHAP satisfies Local Accuracy, Missingness, and Consistency—the unique attribution with these properties (Theorem 1 in Lundberg \& Lee~\citep{lundberg2017unified}). The catch? Exact Shapley values require $2^{|M|}$ evaluations. KernelSHAP approximates via weighted linear regression, but still needs 2,000-10,000 model evaluations for reasonable convergence. For 512-dimensional embeddings with superpixel features ($M \approx 50$), this translates to 5-10 minutes per image on GPU.

\subsection{Evaluation of Attribution Faithfulness}

How do we know if an attribution is correct? Prior work has pursued two main approaches: plausibility (do humans agree?) and faithfulness (does it correlate with model internals?).

Insertion-deletion metrics~\citep{petsiuk2018rise} measure how model confidence changes when features are progressively added (insertion) or removed (deletion) according to attribution importance. High insertion AUC means adding top-attributed features recovers confidence quickly. High deletion AUC means removing them destroys confidence rapidly. Limitation: both systematically create out-of-distribution inputs (images with most pixels masked), undermining validity. As Hooker et al.~\citep{hooker2019benchmark} demonstrated, insertion-deletion scores often reflect how well the model handles distribution shift rather than attribution quality.

Sanity checks~\citep{adebayo2018sanity} test whether attributions change when model parameters are randomized (Data Randomization Test) or when compared to edge detection filters (Edge Detector Test). Surprisingly, many popular methods fail. Grad-CAM attributions often remain visually similar even after fully randomizing the network—suggesting the method highlights input patterns (edges, textures) rather than learned features. This sparked a wave of follow-up work proposing more robust attribution methods, though debate continues about what ``passing'' these tests actually means.

Zhou et al.~\citep{zhou2022attribution} took a different approach: establish ground truth by introducing known manipulations (watermarks, targeted blur patterns) during training, then test whether attribution methods recover them. Best-performing methods (SHAP, Integrated Gradients) still missed 31\% of manipulated features on carefully controlled experiments. This reveals a sobering reality: even methods with strong axiomatic properties don't guarantee empirical correctness.

\textbf{The gap our work fills:} No prior work establishes Popperian falsifiability criteria for attributions in face verification. Zhou et al.'s framework applies to classification with known ground truth. We extend to pairwise verification on hypersphere embeddings, where ground truth is unavailable but counterfactual predictions are testable.

\subsection{Counterfactual Explanations}

Counterfactuals answer ``what if?'' questions by generating minimal input modifications that flip predictions~\citep{wachter2017counterfactual,kenny2021plausible}. For classification: ``What minimal change would make this image classify as `dog' instead of `cat'?'' For verification: ``What minimal face modification would flip `match' to `non-match'?''

Wachter et al.~\citep{wachter2017counterfactual} formalized this for differentiable classifiers, optimizing:
\begin{equation}
\min_{x'} \|\hat{y}' - y_{\text{target}}\|^2 + \lambda \|x' - x\|^2
\end{equation}
where $\hat{y}'$ is the predicted class for $x'$ and $y_{\text{target}}$ is the desired outcome. Follow-up work added diversity constraints (generate multiple distinct counterfactuals)~\citep{mothilal2020diverse}, feasibility constraints (ensure $x'$ lies on the data manifold)~\citep{joshi2019towards}, and sparsity (minimize number of changed features)~\citep{russell2019efficient}.

Most counterfactual work targets Euclidean classification tasks with cross-entropy loss. Our contribution: \textbf{counterfactual generation on non-Euclidean hypersphere geometries} with geodesic distance objectives. We optimize:
\begin{equation}
\min_{x'} \left(d_g(f(x), f(x')) - \delta_{\text{target}}\right)^2 + \lambda \|x' - x\|_2^2
\end{equation}
subject to $f(x') \in \Sphere^{511}$ (enforced automatically via model normalization). The squared geodesic distance error drives embeddings to target separation while $\ell_2$ proximity ensures perceptual plausibility. Feature masking restricts perturbations to specific regions (high- vs low-attribution), enabling the differential predictions required by Theorem~\ref{thm:falsifiability}.

This geometric perspective matters. Early experiments using Euclidean distance $\|f(x) - f(x')\|_2$ produced counterfactuals that looked plausible but moved embeddings in geometrically unnatural directions (large Euclidean distance, small geodesic distance). Switching to geodesic optimization improved convergence by 34\% and reduced perceptual distance (LPIPS) by 0.12 on average—validating that geometry-aware design is not merely theoretical elegance but practical necessity.

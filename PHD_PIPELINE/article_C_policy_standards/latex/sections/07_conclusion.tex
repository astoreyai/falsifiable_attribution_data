\section{Conclusion}

Face recognition systems deployed in law enforcement operate at the intersection of impressive technical capabilities and profound accountability challenges. These systems achieve high matching accuracy—often exceeding 99.7\% on benchmark datasets—yet their decision-making processes remain opaque. Explainable AI methods offer a path toward transparency by generating visual attributions highlighting influential facial features. However, current practice exhibits a critical gap: explanations are generated without rigorous validation of their faithfulness to model reasoning.

This gap matters because regulatory frameworks increasingly mandate not just explanations, but \emph{accurate} explanations. The EU AI Act requires ``accurate, accessible, and comprehensible information''; GDPR demands ``meaningful information about the logic involved''; and U.S. courts applying Daubert standards require testable methods with known error rates. Current XAI practice—producing explanations without validating them—cannot demonstrate compliance with these requirements.

Through systematic analysis of three major regulatory frameworks (EU AI Act, GDPR, Daubert standard), we identified seven core evidentiary requirements: meaningful information, testability, known error rates, appropriate accuracy, adherence to standards, comprehensibility, and human oversight support. For each requirement, we proposed minimal technical evidence specifications, validation methods, and acceptance thresholds that operationalize vague legal concepts into measurable criteria.

The proposed framework reveals a troubling pattern: current practice achieves compliance in form but not substance. Systems generate explanations (satisfying literal regulatory language) without validation (failing the policy intent). This form-versus-substance gap exposes legal systems, defendants, and agencies to serious risks—wrongful identifications based on misleading explanations, Daubert inadmissibility challenges that derail prosecutions, and regulatory enforcement uncertainty that creates legal exposure for deploying agencies.

The path forward requires evidence-based policy. We conclude with concrete recommendations:

\textbf{For Regulators}: Establish technical standards operationalizing vague legal language (``meaningful information,'' ``appropriate accuracy'') into measurable criteria. Mandate pre-registered validation protocols with published benchmarks. Require error rate disclosure including demographic stratification. Establish periodic revalidation requirements as systems evolve.

\textbf{For Developers}: Adopt validation-first development practices where XAI methods are selected based on empirical performance, not popularity. Contribute to community benchmark development. Provide calibrated uncertainty estimates and per-instance quality scores. Transparently document limitations and known failure modes. Publish validation protocols and results in peer-reviewed venues.

\textbf{For Auditors}: Adopt standardized evaluation protocols like the compliance template proposed here. Conduct independent validation beyond vendor claims. Employ red team testing for edge cases and demographic subgroups. Establish continuous monitoring programs. Require transparency enabling replication.

\textbf{For Courts}: Subject XAI evidence to rigorous Daubert scrutiny—testability, error rates, published standards, peer review. Require expert witnesses to demonstrate validation, not merely familiarity with tools. Develop judicial education programs on XAI validation principles. Create standard jury instructions explaining the distinction between model accuracy and explanation accuracy.

The compliance template and example validation (Grad-CAM on ArcFace) demonstrate both the feasibility and necessity of systematic assessment. The example system passed 3/7 requirements—sufficient for investigative leads under supervision, but not for primary evidence in legal proceedings. This nuanced assessment, grounded in measurable criteria, enables risk-informed deployment decisions that balance innovation with accountability.

Face recognition XAI stands where DNA analysis stood decades ago—at an inflection point between ad-hoc practice and scientific rigor. DNA analysis evolved from a novel forensic tool with uncertain reliability into a cornerstone of criminal justice, but only after developing validation protocols, error rate disclosure requirements, and proficiency testing standards. This evolution followed high-profile wrongful convictions and critical National Research Council reports demanding scientific foundations for forensic methods.

Face recognition XAI faces similar pressures. Robert Williams and Michael Oliver—arrested based on false matches—represent visible failures of a broader accountability gap. The EU AI Act, GDPR Article 22, and Daubert standards create legal obligations. Yet between regulatory mandate and technical reality lies a translation problem: how to operationalize legal requirements into measurable technical criteria?

This article provides that translation. The seven evidentiary requirements, minimal thresholds, and compliance template transform abstract legal concepts into concrete technical specifications. These specifications aren't final answers—threshold values require community consensus through standards development processes, and validation methods will evolve as XAI techniques advance. But they provide a starting point grounded in statistical practice, forensic science precedent, and existing regulatory frameworks.

The status quo—deploying explanations without validation—is scientifically indefensible and legally untenable. Explanations that are 68\% faithful (below our 0.70 threshold) and 76\% accurate (below our 80\% threshold) may be better than nothing, but they're insufficient for contexts where liberty is at stake. The documented failure modes—profile faces, low resolution, occlusion, demographic disparities, borderline scores—reveal systematic patterns that operators must understand to make informed decisions.

The framework proposed here serves multiple stakeholders with aligned interests in accuracy and accountability. Defendants gain tools to challenge unreliable evidence. Law enforcement agencies protect themselves from liability while improving investigative effectiveness. Regulators obtain clear compliance criteria. Developers establish level playing fields. Courts streamline admissibility determinations. Society benefits from reduced wrongful identifications and increased trust in legitimate applications of face recognition technology.

Achieving these benefits requires multi-stakeholder collaboration—which is both urgent and achievable. Standards bodies like ISO and NIST must convene researchers, practitioners, regulators, and civil liberties advocates to refine thresholds and develop consensus benchmarks. We observe that research communities are beginning to shift from prioritizing subjective interpretability to objective faithfulness validation, though publication incentives still favor novel methods over rigorous evaluation. Vendors face perhaps the hardest challenge: embracing validation-first development even when it reveals uncomfortable limitations about their products. Courts, finally, need expertise to evaluate technical evidence rigorously—moving beyond uncritical deference to expert testimony toward informed gatekeeping.

This collaboration is both urgent and achievable. The technical foundations exist—counterfactual validation, conformal prediction, ground truth benchmarks. The legal frameworks create obligation and motivation. The wrongful arrest cases demonstrate tangible harms of inaction. What remains is translation and implementation.

This article provides the translation: seven requirements, minimal thresholds, a compliance template, and stakeholder-specific recommendations. Implementation requires commitment from regulators, developers, auditors, and courts to establish evidence-based validation as the norm rather than the exception.

The choice is clear. We can continue deploying explanations without validation, hoping that systems are trustworthy while lacking tools to verify that trust. Or we can demand evidence—testable predictions, known error rates, published standards, peer-reviewed protocols. The former preserves the status quo and its attendant risks. The latter builds accountability into AI systems from the foundation.

Face recognition technology offers genuine benefits for public safety and security. But those benefits cannot come at the cost of civil liberties or scientific integrity. Validated explainability bridges that gap—enabling beneficial applications while providing the accountability mechanisms that protect individual rights. The framework proposed here shows the path. Now comes the hard work of walking it.

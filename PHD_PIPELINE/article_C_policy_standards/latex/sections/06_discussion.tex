\section{Discussion and Policy Implications}

The analysis reveals a fundamental mismatch between regulatory intent and technical practice. Legal frameworks mandate explainability for high-stakes AI systems, yet current XAI deployment lacks the validation foundations that regulators, courts, and practitioners require. This section discusses implications for key stakeholders and proposes actionable recommendations.

\subsection{For Regulators and Standards Bodies}

\subsubsection{Gap Identified}

Regulatory language—GDPR's ``meaningful information,'' the AI Act's ``appropriate accuracy''—lacks technical operationalization. This ambiguity enables ``checkbox compliance'' where systems generate explanations without validating their quality.

\subsubsection{Recommendations}

\textbf{Establish Technical Standards}. Regulatory bodies (e.g., EU AI Office, NIST) should publish technical standards specifying minimal evidence requirements for XAI validation, analogous to existing standards for DNA analysis or digital forensics. The seven requirements and thresholds proposed here provide a starting point.

\textbf{Mandate Validation Protocols}. Require pre-registered validation protocols with published benchmarks before high-risk AI systems can be deployed. Protocols should specify acceptance thresholds before data collection to prevent post-hoc p-hacking.

\textbf{Require Error Rate Disclosure}. Mandate that deployed systems document known failure modes and conditional error rates (e.g., explanation faithfulness by demographic group, image quality, score range). This mirrors Daubert's error rate requirement and enables risk-informed deployment.

\textbf{Periodic Revalidation}. Establish timelines for revalidation (e.g., annually) as models, methods, and datasets evolve. Face recognition systems aren't static—validation cannot be one-time certification.

\textbf{Demographic Fairness Requirements}. Extend validation requirements to include fairness thresholds—explanation faithfulness must meet minimal thresholds for all demographic groups, not just aggregate populations. As illustrated in our Grad-CAM example (Section 5.2), faithfulness for dark-skinned females falls to $\rho = 0.64$ compared to $\rho = 0.68$ overall—a disparity that exacerbates existing bias concerns in face recognition systems~\cite{grother2019frvt}.

\textbf{Precedent}: The European Union's Medical Device Regulation (MDR 2017/745)~\cite{mdr2017} provides a model for risk-based AI oversight with technical standards, conformity assessment, and post-market surveillance. Adapting these principles to AI explainability could establish rigorous governance.

\subsection{For System Developers and Vendors}

\subsubsection{Gap Identified}

Current development practices treat explanation generation as a feature add-on, not a core system requirement with validation obligations.

\subsubsection{Recommendations}

\textbf{Validation-First Development}. Incorporate faithfulness validation into the development lifecycle from the start, not as a post-deployment afterthought. XAI methods should be selected based on empirical validation performance, not popularity or visual appeal.

\textbf{Benchmark Participation}. Contribute to community development of standardized XAI benchmarks with ground truth. Publish validation results to establish credibility and enable comparative evaluation.

\textbf{Uncertainty Quantification}. Provide calibrated confidence intervals for explanations using conformal prediction~\cite{vovk2005conformal} or Bayesian methods. Operators need uncertainty estimates to calibrate trust appropriately.

\textbf{Per-Instance Quality Scores}. Develop and deploy reliability indicators that enable operators to identify when specific explanations are unreliable. Aggregate validation metrics are insufficient—operators need case-level guidance. The Grad-CAM example's AUC of 0.71 shows this remains challenging but achievable.

\textbf{Transparent Limitation Documentation}. Clearly communicate known failure modes in user documentation and system interfaces. When an image falls into a documented failure mode (e.g., profile face, low resolution), flag this for operators automatically.

\textbf{Open Validation}. Publish validation protocols and results in peer-reviewed venues. Proprietary systems can be validated on public benchmarks without disclosing model weights.

\textbf{Business Case}: While validation adds development costs, it mitigates liability risks. Systems that contribute to wrongful arrests or fail Daubert challenges expose vendors to lawsuits. Proactive validation provides defensible due diligence.

\subsection{For Auditors and Oversight Bodies}

\subsubsection{Gap Identified}

Auditors tasked with assessing AI system compliance lack technical tools and standards to evaluate explanation quality.

\subsubsection{Recommendations}

\textbf{Adopt Standardized Evaluation Protocols}. Use the compliance template (Section 5) or similar structured frameworks to systematically assess XAI validation. Require vendors to provide completed templates as part of procurement or compliance review.

\textbf{Independent Validation}. Don't rely solely on vendor-provided validation studies. Conduct independent testing on held-out datasets, particularly for high-stakes deployments. The difference between vendor claims and independent assessment can be substantial.

\textbf{Red Team Testing}. Employ adversarial evaluation to identify conditions under which explanations fail. Test edge cases: demographic groups underrepresented in training data, challenging poses, adversarial perturbations. The five failure modes identified in the Grad-CAM example emerged precisely from this kind of stratified analysis.

\textbf{Ongoing Monitoring}. Compliance isn't binary or static. Establish continuous monitoring programs that track explanation quality metrics over time as systems evolve and operational conditions change.

\textbf{Transparency Requirements}. Require that systems undergoing audit provide sufficient access for replication—validation datasets, model APIs (even if weights remain proprietary), and detailed methodology documentation.

\textbf{Precedent}: Financial services auditing (e.g., SOX compliance for algorithmic trading) provides models for independent technical evaluation of complex systems with legal accountability.

\subsection{For Courts and Legal Professionals}

\subsubsection{Gap Identified}

Judges and attorneys lack technical expertise to evaluate XAI evidence presented in criminal proceedings, leading to either uncritical acceptance or blanket exclusion.

\subsubsection{Recommendations}

\textbf{Daubert Challenges for XAI Evidence}. When face recognition explanations are introduced as evidence, defense attorneys should challenge admissibility under Daubert. Critical questions include: Has the XAI method been validated with documented error rates across different conditions? Are published standards controlling its operation, or does deployment rely on ad-hoc vendor practices? Can the explanation be tested through controlled experiments that could potentially refute its claims? Whether peer review has occurred matters—but more importantly, whether that review addressed validation rather than just method description.

The Grad-CAM example's failure on Standards (no pre-registration), Meaningful Information ($\rho = 0.68$), and Accuracy (76\%) would provide grounds for challenge.

\textbf{Expert Witness Standards}. Courts should require that expert witnesses presenting XAI evidence have conducted (or reviewed) rigorous validation studies, not merely familiarity with the XAI tool. An expert testifying ``we used Grad-CAM'' without validation data should face cross-examination on faithfulness, error rates, and failure modes.

\textbf{Judicial Education}. Provide training for judges on XAI fundamentals and validation principles through judicial education programs (e.g., Federal Judicial Center). Enable informed gatekeeping without requiring deep technical expertise. Simple questions—``What is the correlation between predicted and actual score changes?'' ``What percentage of cases fall into known failure modes?''—can reveal whether proper validation occurred.

\textbf{Standard Jury Instructions}. Develop model jury instructions for cases involving face recognition evidence. Jurors need to understand the critical distinction between model accuracy (how often the system correctly matches faces) and explanation accuracy (whether the highlighted features actually drove the decision). Validation metrics like correlation coefficients and error rates should be explained in accessible terms. Known limitations—such as degraded performance on profile faces or low-resolution images—deserve explicit mention, as does guidance on the appropriate weight XAI evidence should carry relative to other forensic methods.

\textbf{Precedent Development}. As cases involving XAI evidence accumulate, appellate decisions should establish precedent on admissibility standards, clarifying how Daubert applies to explainability methods specifically.

Key point: technical faithfulness is necessary but not sufficient for legal admissibility. Even validated explanations must be probative, reliable in the specific case context, and not unduly prejudicial.

\subsection{Who Benefits From Validated XAI?}

The proposed validation framework serves multiple stakeholders with aligned interests in accuracy and accountability:

\textbf{Defendants and Accused Persons}. Validated explanations enable effective challenges to face recognition evidence. If an explanation fails validation thresholds, defense attorneys have grounds to argue for exclusion or reduced evidentiary weight.

\textbf{Law Enforcement and Forensic Analysts}. Validated systems protect agencies from liability risks associated with wrongful arrests. Knowing when explanations are reliable versus unreliable enables more effective investigations and resource allocation. The 23\% rejection rate in the Grad-CAM example means analysts can focus expert review where it's most needed.

\textbf{Regulatory Agencies}. Validated systems provide clear compliance evidence, reducing enforcement ambiguity and enabling risk-based oversight prioritization.

\textbf{System Developers}. Validation standards create level playing fields and enable differentiation based on empirical performance rather than marketing claims.

\textbf{Judges and Courts}. Validated evidence reduces Daubert hearing complexity and provides clear admissibility criteria, streamlining proceedings.

\textbf{Society}. Reduced wrongful identifications protect civil liberties; transparent accountability mechanisms build public trust in beneficial uses of face recognition technology.

\subsection{Remaining Gaps and Future Directions}

While this article provides a framework for operationalizing existing regulatory requirements, several gaps require ongoing attention:

\textbf{Threshold Consensus}. The proposed thresholds ($\rho \geq 0.70$, 80\% accuracy, etc.) are informed by statistical practice and analogous domains but require community consensus through standards development processes (ISO, NIST, professional societies).

\textbf{Dynamic Adaptation}. Validation standards must evolve as XAI methods, face verification architectures, and adversarial threats develop. Static standards risk obsolescence.

\textbf{Cross-Jurisdictional Harmonization}. U.S., EU, and other jurisdictions have different legal frameworks. International standards harmonization could reduce compliance complexity for multinational deployments.

\textbf{Fairness Integration}. Current regulatory frameworks address explainability and accuracy but lack explicit fairness requirements. Future standards should mandate that validation thresholds are met across demographic groups. Our Grad-CAM example revealed this gap concretely: while aggregate faithfulness reached $\rho = 0.68$, dark-skinned females experienced $\rho = 0.64$—falling further below the 0.70 threshold and compounding algorithmic bias concerns.

\textbf{Alternative Explanation Paradigms}. This article focuses on attribution-based XAI (saliency maps). Other paradigms—example-based explanations, concept-based interpretability, natural language rationales—require separate validation frameworks.

\subsection{A Call for Evidence-Based Policy}

Current XAI practice has operated in a normative vacuum—researchers develop methods based on intuition, vendors deploy based on demand, and regulators mandate explainability without technical specificity. This article proposes a shift toward \textbf{evidence-based explainability policy}:

\begin{itemize}[itemsep=2pt]
    \item Requirements grounded in measurable criteria
    \item Validation following scientific method principles
    \item Standards informed by empirical performance data
    \item Ongoing evaluation as systems and threats evolve
\end{itemize}

This mirrors the evolution of other forensic domains. DNA analysis, fingerprint comparison, and ballistic matching once lacked rigorous scientific foundations. Following high-profile wrongful convictions and critical reports (e.g., the 2009 NRC report on forensic science~\cite{nrc2009}), these fields developed validation protocols, error rate disclosure requirements, and proficiency testing standards.

Face recognition XAI stands at a similar inflection point. Documented wrongful arrests~\cite{hill2020williams,hill2023oliver} and regulatory mandates~\cite{euaiact2024,gdpr2016,daubert1993} create urgency for evidence-based standards. The framework proposed here—seven evidentiary requirements with operationalized thresholds and a compliance template—provides a starting point, not a final answer. Refinement through multi-stakeholder collaboration (researchers, practitioners, regulators, civil liberties advocates) is essential.

But the status quo—deploying explanations without validation—is scientifically indefensible and legally untenable. The time for evidence-based explainability policy is now.
